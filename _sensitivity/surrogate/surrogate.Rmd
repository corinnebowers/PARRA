---
title: "surrogate"
output:
  html_document:
    toc: true 
    toc_float: true
    #toc_depth: 3  
    code_folding: hide
    number_sections: true 
    theme: spacelab   #https://www.datadreaming.org/post/r-markdown-theme-gallery/
    highlight: tango  #https://www.garrickadenbuie.com/blog/pandoc-syntax-highlighting-examples/
---

The goal of this script is to fit a computationally efficient surrogate model to predict the LISFLOOD inundation map based only on the inputs $Q_p$ (peak flow, m^3/s) and $t_p$ (time to peak flow, hrs). 

```{r setup, include = FALSE}
## setup
knitr::opts_knit$set(root.dir = 'D:/1-PARRA/')
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(fig.show = 'hold', fig.align = 'center')

```

```{r message = FALSE}
## setup information
source('_data/setup.R')

## set parallel backend
num_cores <- 5

## load location information
load('_data/lisflood/dem.Rdata')

## load catalog
load('_data/catalog/catalog.Rdata')

```


# Create model space of LISFLOOD simulations

First we generate 5,000 Latin hypercube samples of $Q_p$ and $t_p$ and generate .bci  and .bdy files for LISFLOOD analysis using the script $generate_files.sh$ in Sherlock. 
This populates the model search space so that the surrogate model can always find a combination of parameters ``close'' to the one of interest.  
We then run the script $run_lisflood.sh$ to generate .par files and inundation maps for each parameter combination. 

```{r good, echo = FALSE}
#### define samples within surrogate model space ####

## define number of expected samples
n <- 1000

## remove simulations that errored out in Sherlock
files <- list.files('_sensitivity/surrogate/sherlock_grid/results')
sims <- files %>% gsub('gridded', '', .) %>% gsub('.max', '', .) %>% toNumber
bad1 <- (1:n)[!(1:n %in% sims)]

## remove simulations that do not reach the ocean
pt <- data.frame(lat = 38.45166, lon = -123.12934) %>% 
  st_as_sf(coords = c('lon', 'lat'), crs = 4326) %>% 
  st_transform(6417) %>% 
  st_coordinates
filenames <- list.files('_sensitivity/surrogate/sherlock_grid/results', full.names = TRUE)
start <- Sys.time()
pb <- txtProgressBar(min = 0, max = length(files), style = 3)
cl <- parallel::makeCluster(round(detectCores()*2/3))
registerDoSNOW(cl)
badfile <-
  foreach (
    file = filenames, .combine = 'c',
    .options.snow = list(progress = function(n) setTxtProgressBar(pb, n)),
    .packages = c('raster', 'terra', 'sf', 'dplyr'), .inorder = FALSE) %dopar% {
      if (rast(file) %>% terra::extract(pt) == 0) which(file == filenames)
    }
stopCluster(cl)
Sys.time() - start
bad2 <- files[badfile] %>% 
  str_remove('gridded') %>% str_remove('.max') %>% toNumber %>% sort
# write.table(bad2, file = 'C:/Users/cbowers/Desktop/id.txt',
#             row.names = FALSE, col.names = FALSE)

## keep successful simulations
if (length(bad1) == 0 & length(bad2) == 0) {
  good <- 1:n
} else {
  good <- (1:n)[-c(bad1, bad2)]
}

```



We check the `r comma_format(n)` simulations and disqualify those that either (a) do not produce an inundation map or (b) produce inundation maps where the flow does not fill the river channel all the way to the Pacific Ocean. 
This leaves us with `r length(good)` valid simulations to use for the surrogate model search space. 

```{r samples}
## load samples
samples <-
  read.table('_sensitivity/surrogate/sherlock_grid/samples_grid.txt', header = TRUE) %>%
  mutate(sim = 1:nrow(.)) # %>% 
  # rbind(c(tp = 1e-6, Qp = 1e-6, sim = n+1))  #add a lower-bound simulation

## plot failed simulations
g <- ggplot(samples %>% arrange(!(sim %in% good))) + 
  geom_point(aes(x = Qp, y = tp, color = !sim %in% good)) + 
  scale_color_manual('Failed \nSimulations', values = c('grey80', 'red')) + 
  scale_x_origin('Peak Flow, Qp (m3/s)', labels = comma) + 
  scale_y_origin('Time to Peak Flow, tp (hrs)', labels = comma)

## compare against real values
g + geom_point(data = catalog, aes(x = Qp_m3s, y = tp_hrs))

## add a CV column
cv <- sample(1:10, size = length(good), replace = TRUE)
samples <- samples %>% mutate(cv = replace(NA, sim %in% good, cv))
# ggplot(samples) + 
#   geom_point(aes(x = Qp, y = tp, color = factor(cv))) + 
#   scale_color_scico_d(palette = 'hawaii') + 
#   scale_x_origin() + scale_y_origin()

## add a lower-bound simulation

## save out for Sherlock
save(samples, file = '_sensitivity/surrogate/checkpoints/samples.Rdata')

```


# Set up the surrogate model

The next step is to define the surrogate model. 
<!-- We have chosen to use the inverse distance weighted (IDW) spatial interpolation method.  -->
We used the inverse distance weighting (IDW) spatial interpolation method described in the equations below to generate inundation maps within the Monte Carlo process and reduce the computational demand of the PARRA framework. 
The IDW method has three hyperparameters to tune, which are as follows:

* $n$ defines the size of the search neighborhood;
* $p$ is the power function coefficient, which defines the decay rate of the distance weighting; and
* $\alpha$ defines the anisotropy (asymmetry of information content) between $Q_p$ and $t_p$.

$$ \mathrm{M}^* = \frac{\sum\limits_{i=1}^n \lambda_i \mathrm{M}_i}{\sum\limits_{i=1}^n\lambda_i} $$ 

$$ \lambda_i = \frac{1}{\lVert \mathrm{\textbf{x}}^*, \mathrm{\textbf{x}}_i \rVert ^p} $$

$$ \mathrm{\textbf{x}} = \left\{ \alpha\!*\!z_{Q_p}, \; (1-\alpha)\!*\!z_{t_p} \right\} $$

$\mathrm{M}^*$ represents the unknown (target) inundation map we are trying to predict. 
We calculate this map as the weighted sum of the $n$ closest maps. 
A map $\mathrm{M}_i$ is defined as ``close'' if its hydrograph parameters $\left\{Q_p, t_p \right\}_i$  are similar to the target hydrograph parameters $\left\{Q_p, t_p \right\}^*$.  
$Q_p^*$ and $t_p^*$ are known values used as inputs to the IDW interpolator function. 
However, they are not Euclidean coordinates, so $\mathrm{\textbf{x}}^*$ is the coordinate vector that represents $\left\{Q_p, t_p \right\}^*$ in modified parameter space.

<!-- %after applying a normal score transformation and an anisotropy correction factor $\alpha$.  -->
We calculate the L2 (Euclidean) distance between $\mathrm{\textbf{x}}^*$, the coordinate vector of the target map, and $\mathrm{\textbf{x}}_i$, the coordinate vector of the $i^{th}$ closest map. 
The inverse of this distance multiplied by the power function $p$ is the calculated weight $\lambda_i$ for map $\mathrm{M}_i$, as shown in Equation \ref{eq:IDW2}. 


# Find best-fit IDW hyperparameters

We find the best-fit hyperparameters by performing 10-fold cross-validation on Sherlock using the script $surrogatemodel_array.R$. 
The results are loaded and analyzed below.

Rather than the rigorous parameter fit process we conducted for the LISFLOOD environmental parameters in $rp100.Rmd$, this is more of a qualitative assessment. 


<!-- * define accuracy metrics -->
<!-- * run in Sherlock -->
<!-- * input results -->


```{r}
files <- paste0('_sensitivity/surrogate/sherlock_npalpha/error', 1:10, '.csv')
columns <- cols(
  X1 = col_double(),
  sim = col_double(),
  n = col_double(),
  p = col_double(),
  alpha = col_double(),
  SRSS = col_double(),
  RMSE = col_double(),
  max.resid = col_double(),
  max.loc = col_double()
)
error <- foreach(file = files) %do% {
  read_csv(file, col_types = columns) %>% select(sim, n, p, alpha, RMSE, SRSS)} %>% 
  do.call(rbind, .)

```

```{r}
error.npalpha <- error %>% 
  group_by(n, p, alpha) %>% 
  summarize(RMSE.05 = quantile(RMSE, 0.1), 
            RMSE.median = median(RMSE), 
            RMSE.95 = quantile(RMSE, 0.9),
            SRSS = sqrt(sum(SRSS^2)))
temp <- 
  lapply(
    X = c('n', 'p', 'alpha'),
    FUN = function(x) {
      error.npalpha %>% 
        filter(alpha != 0 & alpha != 1) %>% 
        ggplot() + 
        geom_point(aes(x = RMSE.median, y = SRSS, color = get(x))) + 
        scale_color_scico(x, palette = 'roma') +
        scale_x_origin() + scale_y_origin()
    })
do.call(plot_grid, temp)
temp <- 
  lapply(
    X = c('n', 'p', 'alpha'),
    FUN = function(x) {
      error.npalpha %>% 
        filter(alpha != 0 & alpha != 1) %>% 
        ggplot() + 
        geom_point(aes(x = RMSE.05, y = RMSE.95, color = get(x))) + 
        scale_color_scico(x, palette = 'roma') +
        scale_x_origin() + scale_y_origin()
    })
do.call(plot_grid, temp)

```

```{r}
## iteratively remove points
# error.subset <- error.npalpha %>% 
#   filter(alpha > 0.25) %>%
#   filter(p >= 1)
# error.subset <- error.npalpha %>% 
#   filter(alpha >= 0.5 & alpha <= 0.8) %>%
#   filter(n >= 3 & n <= 10) %>%
#   filter(p > 1 & p < 5)
error.subset <- error.npalpha %>% 
  filter(alpha > 0.5 & alpha < 0.8) %>%
  filter(n >= 4 & n <= 6) %>%
  filter(p >= 2 & p <= 3)

# require(plotly)
# plot_ly(
#   data = error.subset,
#   x = ~RMSE.median, y = ~SRSS, color = ~factor(n),
#   colors = scico(n = length(unique(error.subset$n)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~RMSE.median, y = ~SRSS, color = ~factor(p),
#   colors = scico(n = length(unique(error.subset$p)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~RMSE.median, y = ~SRSS, color = ~factor(alpha),
#   colors = scico(n = length(unique(error.subset$alpha)), palette = 'roma'))

# temp <- 
#   lapply(X = c('n', 'p', 'alpha'),
#     FUN = function(x) {
#       ggplot(error.subset) + 
#         geom_point(aes(x = RMSE.median, y = SRSS, color = get(x))) + 
#         scale_color_scico(x, palette = 'roma') +
#         scale_x_origin() + scale_y_origin()})
# do.call(plot_grid, temp)
# temp <- 
#   lapply(X = c('n', 'p', 'alpha'),
#     FUN = function(x) {
#       ggplot(error.subset) + 
#         geom_point(aes(x = RMSE.05, y = RMSE.95, color = get(x))) + 
#         scale_color_scico(x, palette = 'roma') +
#         scale_x_origin() + scale_y_origin()})
# do.call(plot_grid, temp)

ggplot(error.npalpha %>% filter(alpha != 0 & alpha != 1)) + 
  geom_point(aes(x = RMSE.median, y = SRSS), color = 'grey70') + 
  geom_point(data = error.subset, aes(x = RMSE.median, y = SRSS))
ggplot(error.npalpha %>% filter(alpha != 0 & alpha != 1)) + 
  geom_point(aes(x = RMSE.05, y = RMSE.95), color = 'grey70') + 
  geom_point(data = error.subset, aes(x = RMSE.05, y = RMSE.95))

```

Conclusions:

* $\alpha$ controls the tradeoff between better mean performance \& better min/max performance
* once you're in the middle range, $n$ doesn't matter
* $p$ is more sensitive than I initially realized (clearest in the plotly plots, where you can toggle off/on)


# Report surrogate model accuracy 

<!-- Finally we discuss the error vs. the computational advantage of using IDW as a surrogate model.  -->

```{r}
error %>% 
  filter(alpha > 0.5 & alpha < 0.8) %>%
  filter(n >= 4 & n <= 6) %>%
  filter(p >= 2 & p <= 3) %>% 
  group_by(n, p, alpha) %>% 
  summarize(median_RMSE = median(RMSE)) %>% 
  arrange(median_RMSE) %>% 
  .[1,]

```


```{r}
## tack on last sample

samples <- samples %>% rbind(c(tp = 1e-6, Qp = 1e-6, sim = 1001, cv = 1))
save(samples, file = '_sensitivity/surrogate/checkpoints/samples.Rdata')

## update the previous samples.Rdata to be samples_grid.Rdata
## I just copy-pasted the file into sherlock/grid_final and named it gridded1001.max
## need to redo CV randomization!

```

