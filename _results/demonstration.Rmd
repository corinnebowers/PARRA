---
title: "Sonoma County 2019 AR Case Study Demonstration"
author: "Corinne Bowers"
date: "11/22/2021"
output:
  html_document:
    toc: true 
    toc_float: true
    #toc_depth: 3  
    code_folding: hide
    number_sections: true 
    theme: spacelab   #https://www.datadreaming.org/post/r-markdown-theme-gallery/
    highlight: tango  #https://www.garrickadenbuie.com/blog/pandoc-syntax-highlighting-examples/
---

The purpose of this script is to reproduce figures and numeric results for the paper "A Performance-Based Approach to Quantify Atmospheric River Flood Risk" (https://doi.org/10.5194/nhess-2021-337). 
We implement a model-by-model comparison of the PARRA simulations at each pinch point vs. the observed values from a severe 2019 atmospheric river (AR) event in Sonoma County, California. 

Please note: all figures are formatted for publication, therefore certain features may not display correctly in this markdown file. 

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = 'D:/1-PARRA/')
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(results = 'hold', fig.show = 'hold', fig.align = 'center')
rm(list=ls())

```

```{r}
## set random seed for reproducibility
set.seed(2021)

## setup information
source('_data/setup.R')
source('_data/plots.R')

## load required packages
require(dataRetrieval)

## set parallel backend
num_cores <- 5

## load historic catalog
load('_data/catalog/catalog.Rdata')

## load location information
load('_data/lisflood/dem.Rdata')
load('_data/aoi/aoi.Rdata')
load('_data/NHD/NHD.Rdata')

## load building information
load('_data/buildings/buildings.Rdata')
load('_data/foundations/foundations.Rdata')

## load depth-damage relationships
load('_data/depthdamage/depthdamage.Rdata')

```

```{r echo = FALSE}
## should figures be saved out for the publication?
publish <- FALSE

if (!publish) {
  theme_set(
    theme_classic() + theme(
      text = element_text(family = 'Segoe UI', size = 12),
      axis.line = element_line(size = 0.5),
      axis.ticks = element_line(size = 0.5, color = 'black'),
      legend.key.size = unit(0.5, 'cm')))
  }

```

# $f(AR)$

The first component model in the PARRA framework is $f(AR)$, or the representation of AR magnitude. 
Instead of generating simulated realizations, we use the observed maximum IVT and duration from the 2019 event as inputs to the next component model. 

## Define area of interest

Figure 2 in the paper shows the study area as a two-part figure.
The first panel is a map of Sonoma County, with important cities/towns and rivers/creeks identified and with the study area marked by a shaded rectangle.
The second plot is a map of California with the Russian River and Sonoma County highlighted for geographic context.
Additional text and callout lines were added outside of R through the image editing software Inkscape.

### Plot Sonoma County

```{r}
## load California county polygons
california <- counties(state = 'CA', class= 'sf') %>% st_transform(6417)

## define highlighted cities within Sonoma County
cities <- matrix(
  c(38.507930, -122.985860, 'Guerneville', 1, 
    38.616588, -122.858989, 'Healdsburg', 2, 
    38.46539312043779, -123.00905110596939, 'Monte Rio', 1, 
    38.708491304036954, -122.9028864411151, 'Geyserville', 1,
    38.47356804731463, -122.89069701384281, 'Forestville', 1), 
  byrow = TRUE, ncol = 4) %>%
  data.frame %>%
  setNames(c('lat', 'long', 'city', 'importance')) %>%
  mutate(lat = toNumber(lat), long = toNumber(long)) %>%
  st_as_sf(coords = c('long', 'lat'), crs = 4269) %>% 
  st_transform(6417)

## plot figure 2a: Sonoma County study area
```

```{r fig2a, echo = FALSE}
ggplot() + 
  geom_sf(data = st_union(sonoma), fill = 'grey90', color = NA, alpha = 0.5) +
  geom_sf(data = aoi, fill = 'grey70', color = 'grey40', alpha = 0.35) +
  geom_sf(data = st_union(sonoma), fill = NA, color = 'grey25', size = 0.75) +
  geom_sf(data = russian %>% summarize(GNIS_Name = GNIS_Name[1]) %>%
            st_transform(6417) %>% st_crop(st_union(sonoma)),
          color = scico(5, palette = 'roma')[5], size = 1) +
  geom_sf(data = creeks %>% st_intersection(st_union(sonoma)),
          color = scico(5, palette = 'roma')[5]) +
  geom_sf(data = cities, aes(size = importance), show.legend = FALSE) + 
  scale_size_manual(values = c(1.5, 1.5)) + 
  annotation_scale(width_hint = 0.25, height = unit(0.25, 'cm'),
                   pad_x = unit(0.5, 'cm'), pad_y = unit(0.5, 'cm'),
                   text_family = 'Segoe UI', text_cex = 2/3) + 
  theme_void() 
if (publish) ggsave('_figures/fig02/fig02_sonoma.png', width = 8.5, units = 'cm', dpi = 600)

```

### Plot California

```{r}
## plot figure 2b: California
```

```{r fig2b, echo = FALSE}
california <-
  california %>% st_union %>% st_sf %>% st_cast('POLYGON') %>% .[1,] %>% 
    st_intersection(california %>% st_cast('POLYGON'), .)

ggplot() + 
  geom_sf(data = california, fill = 'grey90', color = 'grey40', alpha = 0.5, size = 0.25) + 
  geom_sf(data = california %>% filter(NAME == 'Sonoma'),
          fill = 'grey50', color = 'grey25', size = 0.5) +
  geom_sf(data = st_union(california), fill = NA, color = 'grey25', size = 0.5) + 
  theme_void() 
ggsave('_figures/fig02/fig02_california.png', width = 3, units = 'cm', dpi = 600)

```

## Generate historic catalog

We first start by generating a historic catalog of ARs that have occurred in the Sonoma County study area over a 32-year period from 1997 to 2019. 
<!-- All data sources and calculations can be found in the script `catalog.Rmd`. -->
Results are summarized in Table 1 below.

```{r tab1, echo = FALSE}
catalog %>% 
  group_by(cat) %>% 
  summarize(
    n = length(AR),
    ivt = mean(IVT_max), 
    dur = mean(duration), 
    prcp = mean(precip_mm), 
    qp = mean(Qp_m3s), 
    tp = Mean(tp_hrs)) %>% 
  gt %>% 
  fmt_number(c(cat, n), decimals = 0) %>% 
  fmt_number(c(ivt, dur, prcp, qp, tp), decimals = 1) %>% 
  cols_label(
    cat = 'Category', 
    n = 'Events', 
    ivt = 'IVT (kg/m/s)',
    dur = 'Duration (hrs)',
    prcp = 'Precipitation (mm)', 
    qp = 'Peak Streamflow (m^3/s)', 
    tp = 'Time to Peak Streamflow (hrs)') %>% 
  tab_header(title = 'Sonoma County Historic Catalog', 
             subtitle = 'Mean Values by AR Intensity Category') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

## Identify case study AR

We have identified the 2019 AR event as well-suited for a case study for two reasons: first because it was a relatively extreme event that caused significant flooding and damage along the lower Russian River, and second because it was recent enough that we have validation data available for almost every pinch point in the PARRA framework.
The table below shows some of the values recorded in Sonoma County due to this AR event. 

```{r}
## identify 2019 case study AR event 
casestudy <- catalog %>% filter(start_day == ymd('2019-02-25'))

## show the 2019 event variables as a formatted table
```

```{r echo = FALSE}
casestudy %>% 
  select(start_day, end_day, cat, IVT_max, duration, precip_mm, runoff_mm, Qp_m3s, tp_hrs, sm) %>% 
  gt %>%
  fmt_date(columns = c(start_day, end_day), date_style = 'iso') %>%
  fmt_number(columns = c(cat, IVT_max, duration), decimals = 0) %>% 
  fmt_number(columns = c(precip_mm, runoff_mm, Qp_m3s, tp_hrs, sm)) %>% 
  cols_label(
    start_day = 'Start Day', end_day = 'End Day', cat = 'AR Category',
    IVT_max = 'Maximum IVT (kg/m/s)', 
    duration = 'Storm Duration (hrs)', 
    precip_mm = 'Total Precipitation (mm)', 
    runoff_mm = 'Total Runoff (mm)', 
    Qp_m3s = 'Peak Streamflow (m^3/s)', 
    tp_hrs = 'Time to Peak Streamflow (hrs)',
    sm = 'Antecedent Soil Moisture (mm/m)') %>% 
  tab_header(title = '2019 Case Study AR Event') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

## Plot AR characteristics from the historic catalog

We summarize ARs in terms of two variables: maximum recorded integrated water vapor transport (IVT, kg/m/s) and duration (hrs).
Figure 3 from the paper summarizes the maximum IVT and duration values from the Sonoma County historic catalog and highlights the 2019 case study AR in relation to the rest of the catalog.
The bottom and left histograms represent the marginal distributions of maximum IVT and duration, respectively. 
The scatterplot shows the joint distribution of these two variables for all ARs in the historic catalog.
The background colors show the AR intensity categories from Ralph et al. (2020), and the dashed bullseye lines represent the 2019 case study event. 

```{r}
## create dataframe to represent AR intensity categories as background colors
IVT_breaks <- seq(250, 1250, 250) + 125
duration_breaks <- seq(0, 168, 24) + 12
df <- expand.grid(IVT = IVT_breaks, duration = duration_breaks)
ARcat <- function(IVT, duration) {
  if (duration >= 48) {
    case_when(
      IVT>=1000 ~ 5, IVT>=750 ~ 4, IVT>=500 ~ 3, TRUE ~ 2)
  } else if (duration >= 24) {
    case_when(
      IVT>=1250 ~ 5, IVT>=1000 ~ 4, IVT>=750 ~ 3, IVT>=500 ~ 2, TRUE ~ 1)
  } else {
    case_when(
      IVT>=1250 ~ 4, IVT>=1000 ~ 3, IVT>=750 ~ 2, IVT>=500 ~ 1, TRUE ~ 0)
  }
}
for (i in 1:nrow(df)) {df$cat[i] <- ARcat(df$IVT[i], df$duration[i])}
df <- df %>% mutate(cat = ifelse(cat == 0, 1, cat))

## plot figure 3: maximum IVT vs. duration for the historic catalog
```

```{r fig3, echo = FALSE}
## scatterplot of IVT vs. duration
g <- ggplot(df) + 
  geom_raster(aes(x = IVT, y = duration, fill = factor(cat)), alpha = 0.6) +
  geom_point(data = catalog, aes(x = IVT_max, y = duration), color = 'grey10', size = 0.5) + 
  geom_vline(xintercept = casestudy$IVT_max, linetype = 'dashed') + 
  geom_hline(yintercept = casestudy$duration, linetype = 'dashed') + 
  scale_fill_manual('Intensity \nCategory', values = roma.colors[5:1]) + 
  scale_x_continuous(
    limits = c(250, NA), expand = c(0,0), 
    breaks = seq(250, 1500, 250), minor_breaks = seq(250, 1500, 250)) + 
  scale_y_continuous(
    'Storm Duration (hrs)', expand = c(0,0), 
    breaks = seq(0, 240, 24), minor_breaks = seq(0, 240, 24)) + 
  coord_cartesian(ylim = c(0,192)) + 
  theme(axis.title.x = element_blank(), axis.text.x = element_blank(),
        axis.title.y = element_blank(), axis.text.y = element_blank(),
        plot.margin = margin(2,20,2,2))

## histogram of maximum IVT
g.ivt <- ggplot(catalog) + 
  geom_histogram(aes(x = IVT_max), color = 'black', fill = 'grey90', 
                 bins = sqrt(nrow(catalog)), boundary = 250, size = 0.25) + 
  scale_x_continuous(
    expression(paste('Maximum IVT (kg⋅', m^{-1}, s^{-1}, ')')),
    breaks = seq(0, 1e5, 250), expand = c(0,0), limits = c(250,1500)) + 
  scale_y_origin() + 
  coord_cartesian(xlim = c(250, NA), clip = 'off') + 
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), axis.line.y = element_line(color = NA),
        plot.margin = margin(2,2,2,2))

## histogram of storm duration
g.dur <- ggplot(catalog) + 
  geom_histogram(aes(x = duration), color = 'black', fill = 'grey90',
                 bins = sqrt(nrow(catalog)), boundary = 0, size = 0.25) + 
  scale_x_continuous('Storm Duration (h)', expand = c(0,0), 
                     breaks = seq(0, 240, 24), minor_breaks = seq(0, 240, 24)) + 
  scale_y_origin() +
  coord_flip(xlim = c(0,192), clip = 'off') +
  theme(axis.title.x = element_blank(), axis.text.x = element_blank(), 
        axis.ticks.x = element_blank(), axis.line.x = element_line(color = NA),
        plot.margin = margin(2,2,2,2))

## empty plot (for formatting purposes)
g.void <- ggplot() + theme_void() + 
  theme(plot.margin = margin(2,2,2,2))

## combine plot panels into one figure
plot_grid(g, g.ivt, nrow = 2, align = 'v', axis = 'lr', rel_heights = c(3,1)) %>% 
  plot_grid(
    plot_grid(g.dur, g.void, nrow = 2, rel_heights = c(3,1)), ., 
    align = 'h', rel_widths = c(1,3)) 
if (publish) ggsave('_figures/fig03_AR.png', height = 5.75, width = 8.3, units = 'cm', dpi = 600)

```


# $f(PRCP|AR)$

The second component model is $f(PRCP|AR)$, which estimates precipitation as a function of two AR characteristics, maximum IVT ($IVT$) and duration ($DUR$). 

## Fit precipitation regression model

The precipitation component model uses a weighted least squares (WLS) linear regression to estimate a total precipitation (mm) averaged over the inlet watershed. The form of the equation is as follows: 

$$ PRCP_i = \beta_0 + \beta_1 (IVT_i) + \beta_2 (DUR_i) + \beta_3 (IVT_i* DUR_i) + \sigma_i $$

The WLS regression was chosen to account for the significant heterskedasticity in the data. 
The uncertainty $\sigma_i$ is represented by a mixture model, with 90\% of errors based on the bulk distribution and 10\% of errors based on the 10\% most extreme AR events in the historic catalog.
Model coefficients are displayed below.

```{r}
## load precipitation component model functions
source('_scripts/2_PRCP/PRCP.R')

## fit precipitation regression
fit_precip(catalog)

## display fitted regression values
```

```{r echo = FALSE}
model.prcp$coefficients %>% 
  t %>% as.data.frame %>% 
  gt %>% 
  fmt_number(columns = c(`(Intercept)`, IVT_max, duration, `IVT_max:duration`), n_sigfig = 3) %>% 
  cols_label(
    `(Intercept)` = 'Intercept',
    IVT_max = 'Max IVT', 
    duration = 'Duration', 
    `IVT_max:duration` = 'Max IVT * Duration') %>% 
  tab_header(title = 'Precipitation Regression Coefficients') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

Figures 4(a)-(b) show the regression fit vs. the observed data.
In 4(a) the relationship between maximum IVT and precipitation is shown at three specified duration values (*DUR* = 6, 24, & 72 hours). 
In 4(b) the relationship between duration and precipitation is shown at three specified maximum IVT values (*IVT* = 250, 500, & 1000 kg/m/s).

```{r}
## plot figures
## 4(a): scatterplot of maximum IVT vs. precipitation and 
## 4(b): scatterplot of duration vs. precipitation
```

```{r fig4ab, echo = FALSE}
g1 <- ggplot(catalog) + 
  geom_point(aes(x = IVT_max, y = precip_mm), size = 0.5) + 
  geom_point(data = casestudy, aes(x = IVT_max, y = precip_mm), color = roma.colors[1]) + 
  annotate('text', x = casestudy$IVT_max, y = casestudy$precip_mm, 
    label = '2019', color = roma.colors[1], hjust = 1.2, vjust = 0.25,
    family = 'Segoe UI', fontface = 'bold', size = 8/.pt) + 
  geom_line(aes(x = IVT_max,
    y = catalog %>% select(IVT_max) %>% 
      mutate(duration = 6) %>% predict(model.prcp, .),
    color = '6'), size = 0.75) + 
  geom_line(aes(x = IVT_max,
    y = catalog %>% select(IVT_max) %>% 
      mutate(duration = 24) %>% predict(model.prcp, .),
    color = '24'), size = 0.75) + 
  geom_line(aes(x = IVT_max,
    y = catalog %>% select(IVT_max) %>% 
      mutate(duration = 72) %>% predict(model.prcp, .),
    color = '72'), size = 0.75) + 
  scale_x_continuous(
    expression(paste('Maximum IVT (kg⋅', m^{-1}, s^{-1}, ')')),
    limits = c(250, NA), expand = c(0,0), 
    breaks = seq(250, 1500, 250), minor_breaks = seq(250, 1500, 250)) + 
  scale_y_origin('Storm Total Precipitation (mm)') + 
  scale_color_manual('Duration (h)', 
    values = roma.colors[5:3], 
    breaks = c('72', '24', '6')) + 
  coord_cartesian(clip = 'off') + 
  theme(legend.position = 'bottom', 
        legend.background = element_rect(fill = NA, color = NA),
        legend.text = element_text(margin = margin(0, 0, 0, -4)))

g2 <- ggplot(catalog) + 
  geom_point(aes(x = duration, y = precip_mm), size = 0.5) + 
  geom_point(data = casestudy, aes(x = duration, y = precip_mm), color = roma.colors[1]) + 
  annotate('text', x = casestudy$duration, y = casestudy$precip_mm, 
    label = '2019', color = roma.colors[1], hjust = -0.2, vjust = 0.25,
    family = 'Segoe UI', fontface = 'bold', size = 8/.pt) + 
  geom_line(aes(x = duration,
    y = catalog %>% select(duration) %>% 
      mutate(IVT_max = 250) %>% predict(model.prcp, .),
    color = '250'), size = 0.75) + 
  geom_line(aes(x = duration,
    y = catalog %>% select(duration) %>% 
      mutate(IVT_max = 500) %>% predict(model.prcp, .),
    color = '500'), size = 0.75) + 
  geom_line(aes(x = duration,
    y = catalog %>% select(duration) %>% 
      mutate(IVT_max = 1000) %>% predict(model.prcp, .),
    color = '1000'), size = 0.75) + 
  scale_x_continuous(
    'Storm Duration (hrs)', expand = c(0,0), 
    breaks = seq(0, 240, 24), minor_breaks = seq(0, 240, 24)) + 
  scale_y_origin('Storm Total Precipitation (mm)') + 
  scale_color_manual(expression(paste('Maximum IVT (kg⋅', m^{-1}, s^{-1}, ')')), 
    values = roma.colors[5:3], 
    breaks = c('1000', '500', '250')) + 
  coord_cartesian(clip = 'off') + 
  theme(legend.position = 'bottom', 
        legend.background = element_rect(fill = NA, color = NA),
        legend.text = element_text(margin = margin(0, 0, 0, -4)))

if (!publish) g1; g2

```

## Assess distribution fit

Because we are using the component model for generating new synthetic precipitation realizations, we assessed model fit based on the accuracy in reproducing the entire distribution of precipitation rather than in replicating individual records. 
We present two assessment tools here. 

The first is the quantile plot, which plots the sorted values of one distribution against the sorted values of another and visually checks for linearity. 
This is shown as Figure 4(c) in the paper.

```{r}
## generate one stochastic realization of PRCP for every event in the historic catalog
precip.qq <- 
  generate_precip(
    AR = catalog %>% transmute(n.AR = 1:nrow(.), IVT_max, duration, sm), 
    model.prcp, se.prcp,
    probabilistic = TRUE, n.precip = 1) %>% 
  rename(precip_sim = precip_mm) %>% 
  left_join(catalog %>% transmute(n.AR = 1:nrow(.), precip_obs = precip_mm),
            by = 'n.AR')
precip.max <- max(c(max(precip.qq$precip_sim), max(precip.qq$precip_obs)))

## plot figure 4(c): quantile plot of observed vs. simulated precipitation for the historic catalog
```

```{r fig4c, echo = FALSE}
g3 <- ggplot(precip.qq) + 
  geom_point(aes(x = sort(precip_obs), y = sort(precip_sim)), size = 0.75) + 
  coord_cartesian(xlim = c(NA, precip.max), ylim = c(NA, precip.max)) + 
  scale_x_origin('Observed Precipitation Quantile (mm)') + 
  scale_y_origin('Simulated Precipitation Quantile (mm)') + 
  geom_parity() + coord_fixed(xlim = c(0, precip.max), ylim = c(0, precip.max))
if (!publish) g3 

```

The second is the two-sample Kolmogorov-Smirnov (K-S) test, which is a formal test of fit. 
If the simulated CDF is pulled from the same distribution as the observed CDF, then the distance $d$ between the two follows the K-S distribution, and the following is true: 

$$ \mathrm{P} \left(d > 1.731\sqrt{\frac{2}{N}} \right) \approx 0.05 
\to d_{crit} = 1.731\sqrt{\frac{2}{N}} $$
where $N$ = the number of events in the historic catalog.

We define the distance between the observed and simulated precipitation CDFs as $d_{PRCP}$ and perform a hypothesis test to determine whether $\frac{d_{PRCP}}{d_{crit}} \leq 1$ at a significance level of 1%. 
We repeat this process 100 times to account for the probabilistic nature of the simulation process and report the mean test statistic below.

```{r}
## determine d.crit, the value of the 95% confidence threshold
d.crit <- 1.628 * sqrt(2/nrow(catalog))

## calculate the K-S statistic for 100 simulations
n <- 100
cl <- parallel::makeCluster(num_cores)
registerDoSNOW(cl)
ks.value <- 
  foreach (i = 1:n, 
    .combine = 'c', .packages = c('tidyverse', 'pracma'), 
    .export = 'generate_precip') %dopar% {
      ## generate one realization of precipitation
      precip.ks <- generate_precip(
        AR = catalog %>% transmute(n.AR = 1:nrow(.), IVT_max, duration, sm), 
        model.prcp, se.prcp,
        probabilistic = TRUE, n.precip = 1)
      ## create a dataframe of sorted values 
      sorted <- data.frame(
        precip.obs = sort(catalog$precip_mm),
        precip.sim = sort(precip.ks$precip_mm)) %>% 
        mutate(p = (1:nrow(.))/(nrow(.)+1))
      ## find the maximum distance between the observed CDF and simulated CDF
      data.frame(
        precip = seq(0, min(c(max(sorted$precip.obs), max(sorted$precip.sim))), 
                     length.out = 1000)) %>% 
        mutate(p.obs = interp1(x = sorted$precip.obs, y = sorted$p, xi = precip)) %>% 
        mutate(p.sim = interp1(x = sorted$precip.sim, y = sorted$p, xi = precip)) %>% 
        mutate(d = abs(p.obs-p.sim)) %>% 
        mutate(ks = d/d.crit) %>% 
        arrange(desc(ks)) %>% 
        pull(ks) %>% max
  }
stopCluster(cl)

## report K-S statistic
```

```{r echo = FALSE}
mean(ks.value) %>% as.data.frame %>% 
  setNames('Mean K-S Statistic') %>% 
  gt %>% 
  fmt_number(`Mean K-S Statistic`, decimals = 3) %>% 
  tab_options(column_labels.background.color = '#f2f2f2')

```

```{r include = FALSE}
# ## plot distribution of K-S statistic 
# ggplot() + 
#   geom_histogram(aes(x = ks.value, y = ..density..), 
#     color = 'black', fill = 'grey90', bins = sqrt(length(ks.value))) + 
#   geom_vline(xintercept = 1, linetype = 'dashed', size = 1) + 
#   scale_x_continuous('Kolmogorov-Smirnov Statistic') + 
#   scale_y_origin('Probability of Occurrence')

```

Based on a visual assessment of the quantile plot and the fact that the test statistic is much smaller than 1, we consider the fit of the precipitation component model as sufficient to accurately represent the historic catalog.

## Assess case study performance

Once we have confirmed that the WLS regression model produces an accurate representation of the precipitation distribution, we can also consider how well it is able to reproduce the 2019 event. 
We generate 1,000 realizations of precipitation given the observed maximum IVT and duration from the 2019 event, then compare that distribution to the true observed precipitation. 
The results are included as Figure 4(d) in the paper. 

```{r}
## generate 1,000 probabilistic realizations of PRCP for the 2019 event
precip <- 
  generate_precip(
    AR = casestudy %>% transmute(n.AR = 1, IVT_max, duration, sm), 
    model.prcp, se.prcp,
    probabilistic = TRUE,
    n.precip = 1e3)

## plot figure 4(d): histogram of observed vs. simulated precipitation for the 2019 event
```

```{r fig4d, echo = FALSE}
g4 <- ggplot(precip) + 
  geom_histogram(aes(x = precip_mm, y = ..count../nrow(precip)), 
                 color = 'black', fill = 'grey90', 
                 bins = sqrt(nrow(precip)), boundary = 0, size = 0.25) + 
  geom_vline(xintercept = casestudy$precip_mm, linetype = 'dashed') + 
  annotate(geom = 'text', 
           x = casestudy$precip_mm, y = 0.035, hjust = 0, vjust = -0.5,
           label = '2019 Observed', family = 'Segoe UI', size = 8/.pt,
           angle = 90) + 
  scale_x_origin('Storm Total Precipitation (mm)') + 
  scale_y_origin('Probability of Occurrence') + 
  coord_cartesian(xlim = c(0, precip.max))
if (!publish) g4 

```

```{r fig4, include = publish, echo = FALSE}
## generate figure 4
plot_grid(
  plot_grid(ggplot() + theme_void(), ggplot() + theme_void(), 
            nrow = 1, rel_widths = c(1,1),
    labels = c('(a)', '(b)'), label_fontfamily = 'Segoe UI', label_size = 12,
    label_x = c(0.2, 0.7), label_y = 0.95), 
  plot_grid(get_legend(g1), get_legend(g2), nrow = 1, rel_widths = c(1,1)),
  plot_grid(
    g1 + theme(legend.position = 'none'), g2 + theme(legend.position = 'none'), 
    nrow = 1, align = 'h', rel_widths = c(1,1)),
  plot_grid(
    g3, g4, nrow = 1, align = 'h',
    labels = c('(c)', '(d)'), label_fontfamily = 'Segoe UI', label_size = 12,
    label_x = c(0.2, 0.7), label_y = 0.95), 
  nrow = 4, 
  rel_heights = c(1.25,1,9,10))
ggsave('_figures/fig04_precip.png', width = 12, height = 12, units = 'cm', dpi = 600)

```


# $f(HC)$

This component model represents the distribution of potential antecedent hydrologic conditions.
In this implementation we are using soil moisture as a proxy for antecedent hydrologic conditions.
Soil moisture is measured as the equivalent height of water (mm) in the top meter of the subsurface. 

## Fit lognormal distribution to soil moisture

We found that soil moisture records in the historic catalog were well represented by a lognormal distribution. 
The parameters of the distribution are displayed below, and the empirical vs. modeled distribution is shown in Figure 6(a).

```{r results = 'hide'}
## load antecedent hydrologic condition function(s)
source('_scripts/3_HC/HC.R')

## fit lognormal distribution to soil moisture records in the historic catalog 
fit_soilmoisture(catalog)

## report parameters of lognormal distribution
```

```{r echo = FALSE}
fit.sm %>% 
  t %>% as.data.frame %>% 
  gt %>% 
  fmt_number(c('meanlog', 'sdlog'), n_sigfig = 4) %>% 
  cols_label('meanlog' = 'Lognormal Mean', 'sdlog' = 'Lognormal Std. Dev.') %>% 
  tab_header(title = 'Soil Moisture Parameters') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

```{r}
## plot figure 6(a): observed vs. simulated soil moisture data
```

```{r fig6a, echo = FALSE}
g5 <- data.frame(dx = seq(0, 200, length.out = 1e3)) %>% 
  mutate(fit = dlnorm(dx, meanlog = fit.sm[1], sdlog = fit.sm[2])) %>% 
  ggplot() + 
  geom_histogram(
    data = catalog, 
    aes(x = sm, y = ..density.., fill = 'Observed Historic Catalog'),
    color = 'black', size = 0.25) + 
  geom_line(aes(x = dx, y = fit, size = 'Lognormal Distribution')) + 
  geom_vline(xintercept = casestudy$sm, linetype = 'dashed') +
  annotate(geom = 'text', 
    x = casestudy$sm - 6, y = 0.0125, angle = 90,
    label = '2019 Observed', family = 'Segoe UI', size = 8/.pt) + 
  scale_fill_manual(values = 'grey90') + 
  scale_size_manual(values = 0.5) + 
  scale_x_origin('Antecedent Soil Moisture (mm)') + 
  scale_y_origin('Probability of Occurrence') + 
  theme(legend.title = element_blank(), 
        legend.position = 'bottom',
        legend.margin = margin(0,0,0,0))
if (!publish) g5 

```

```{r include = FALSE}
## find percentile of 2019 event
plnorm(casestudy$sm, meanlog = fit.sm[1], sdlog = fit.sm[2])

```

# $f(Q|PRCP, HC)$

The next component model estimates streamflow ($Q$) as a function of precipitation ($PRCP$) and soil moisture ($HC$). 
From Equation 4 in the paper we know that $Q$ is a function of three parameters.
These are: $Q_p$, peak streamflow; $t_p$, time to peak streamflow; and $m$, a unitless shape parameter.
We discuss each of these in more detail. 

## $Q_p$

### Fit peak streamflow regression model

In order to estimate $Q_p$, we first calculated runoff $R$ using the curve number method, then performed an ordinary least squares regression to predict $Q_p$ as a function of runoff and precipitation using the equation below: 

$$ Q_{p,i} = \beta_0 + \beta_1 (PRCP_i) + \beta_2 (R_i) + \beta_3 (PRCP_i:R_i) + \sigma_i $$ 

Similar to the precipitation model, the uncertainty $\sigma_i$ is once again represented by a mixture model, with 90\% of errors based on the bulk distribution and 10\% of errors based on the 10\% most extreme AR events in the historic catalog.
Model coefficients are shown below.

```{r results = 'hide'}
## load streamflow functions(s)
source('_scripts/4_Q/Q.R')

## fit peak streamflow regression
fit_Qp(catalog)

## display fitted model values
```

```{r echo = FALSE}
model.Qp$coefficients %>% 
  t %>% as.data.frame %>% 
  gt %>% 
  fmt_number(columns = c(`(Intercept)`, precip_mm, runoff_mm, `precip_mm:runoff_mm`), n_sigfig = 3) %>% 
  cols_label(
    `(Intercept)` = 'Intercept',
    precip_mm = 'Precipitation', 
    runoff_mm = 'Runoff', 
    `precip_mm:runoff_mm` = 'Precipitation * Runoff') %>% 
  tab_header(title = 'Peak Streamflow Regression Coefficients') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

### Assess distribution fit

We follow the same process as the precipitation component model to fit and validate this equation. 
We present both a quantile plot for visual inspection in Figure 5(a) and a formal two-sample K-S test to validate the fit of the overall peak streamflow distribution as compared to the historic catalog. 

```{r results = 'hide', echo = FALSE}
## fit lognormal distribution to time to peak streamflow records in the historic catalog 
fit_tp(catalog)

```

```{r}
## generate one stochastic realization of Qp for every event in the historic catalog
runoff.qq <- fit_generate_runoff(
  precip = catalog %>% 
    transmute(n.AR = 1:nrow(.), n.precip = 1, n.hc = 1, IVT_max, duration, precip_mm, sm), 
  catalog = catalog, probabilistic = TRUE)
hydro.qq <- generate_hydrograph(
  runoff = runoff.qq, 
  model.Qp = model.Qp, se.Qp = se.Qp, fit.tp = fit.tp, 
  probabilistic = TRUE) %>% 
  rename(Qp_sim = Qp_m3s) %>% 
  left_join(
    catalog %>% 
      transmute(n.AR = 1:nrow(.), n.precip = 1, n.hc = 1, n.runoff = 1, n.hydro = 1, Qp_obs = Qp_m3s), 
            by = c('n.AR', 'n.precip', 'n.hc', 'n.runoff', 'n.hydro'))
hydro.max <- max(c(max(hydro.qq$Qp_obs), max(hydro.qq$Qp_sim)))

## plot panel 5(a): quantile plot of observed vs. simulated peak streamflow for the historic catalog
```

```{r fig5a, echo = FALSE}
g7 <- ggplot(hydro.qq) + 
  geom_point(aes(x = sort(Qp_obs), y = sort(Qp_sim)), size = 0.75) + 
  geom_parity() + 
  coord_cartesian(xlim = c(NA, hydro.max), ylim = c(NA, hydro.max)) + 
  scale_x_continuous(
    expression(paste('Observed ', Q[p], ' Quantile (', m^{3}, s^{-1}, ')')), 
    expand = expansion(mult = c(0, 0.05)), labels = comma) + 
  scale_y_continuous(
    expression(paste('Simulated ', Q[p], ' Quantile (', m^{3}, s^{-1}, ')')), 
    expand = expansion(mult = c(0, 0.05)), labels = comma)
if (!publish) g7

```

```{r}
## determine d.crit, the value of the 95% confidence threshold
d.crit <- 1.628 * sqrt(2/nrow(catalog))

## calculate the K-S statistic for 100 simulations
n <- 100
cl <- parallel::makeCluster(num_cores)
registerDoSNOW(cl)
ks.value <- 
  foreach (i = 1:n, 
    .combine = 'c', .packages = c('pracma', 'fitdistrplus', 'tidyverse'), 
    .export = c('fit_generate_runoff', 'generate_hydrograph')) %dopar% {
      ## generate one realization of peak streamflow
      runoff.ks <- fit_generate_runoff(
        precip = catalog %>% 
          transmute(n.AR = 1:nrow(.), n.precip = 1, n.hc = 1, 
                    IVT_max, duration, precip_mm, sm), 
        catalog = catalog, probabilistic = TRUE)
      hydro.ks <- generate_hydrograph(
        runoff = runoff.ks, 
        model.Qp = model.Qp, se.Qp = se.Qp, fit.tp = fit.tp, 
        probabilistic = TRUE)
      ## create a dataframe of sorted values 
      sorted <- data.frame(
        Qp.obs = sort(catalog$Qp_m3s),
        Qp.sim = sort(hydro.ks$Qp_m3s)+4) %>% #add in baseflow
        mutate(p = (1:nrow(.))/(nrow(.)+1))
      ## find the maximum distance between the observed CDF and simulated CDF
      data.frame(
        Qp = seq(
          sorted %>% select(-p) %>% apply(2, min) %>% max, 
          sorted %>% select(-p) %>% apply(2, max) %>% min, 
          length.out = 1000)) %>% 
        mutate(p.obs = interp1(x = sorted$Qp.obs, y = sorted$p, xi = Qp)) %>%
        mutate(p.sim = interp1(x = sorted$Qp.sim, y = sorted$p, xi = Qp)) %>% 
        mutate(d = abs(p.obs-p.sim)) %>% 
        mutate(ks = d/d.crit) %>% 
        pull(ks) %>% max
  }
stopCluster(cl)

## report K-S statistic
```

```{r echo = FALSE}
mean(ks.value) %>% as.data.frame %>% 
  setNames('Mean K-S Statistic') %>% 
  gt %>% 
  fmt_number(`Mean K-S Statistic`, decimals = 3) %>% 
  tab_options(column_labels.background.color = '#f2f2f2')

```

```{r include = FALSE}
# ## plot distribution of K-S statistic 
# ggplot() + 
#   geom_histogram(aes(x = ks.value, y = ..density..), 
#     color = 'black', fill = 'grey90', bins = sqrt(length(ks.value))) + 
#   geom_vline(xintercept = 1, linetype = 'dashed', size = 1) + 
#   scale_x_continuous('Kolmogorov-Smirnov Statistic') + 
#   scale_y_origin('Probability of Occurrence')
# 
# ## prove that this method is better than the default unit conversion
# R2(
#   catalog$runoff_mm * (readNWISsite(11463500)$drain_area_va * 1609.344^2) / catalog$duration,
#   catalog$Qp_m3s)
# R2(predict(model.Qp), catalog$Qp_m3s)

```

The test statistic is still under 1, so once again this regression model is sufficient to accurately represent the peak streamflow distribution in the historic catalog.

## $t_p$ 

Like the antecedent soil moisture, records of time to peak streamflow $t_p$ in the historic catalog were found to be well represented by a lognormal distribution. 
The parameters of the distribution are displayed below, and the empirical vs. modeled distribution is shown in Figure 6(b).

```{r results = 'hide'}
## fit lognormal distribution to time to peak streamflow records in the historic catalog 
fit_tp(catalog)

## report parameters of lognormal distribution
```

```{r echo = FALSE}
fit.tp %>% 
  t %>% as.data.frame %>% 
  gt %>% 
  fmt_number(c('meanlog', 'sdlog'), n_sigfig = 4) %>% 
  cols_label('meanlog' = 'Lognormal Mean', 'sdlog' = 'Lognormal Std. Dev.') %>% 
  tab_header(title = 'Time to Peak Streamflow Parameters') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

```{r}
## plot panel 6(b): observed vs. simulated time to peak streamflow
```

```{r fig6b, echo = FALSE}
g6 <- data.frame(dx = seq(0, 60, length.out = 1e3)) %>% 
  mutate(fit = dlnorm(dx, meanlog = fit.tp[1], sdlog = fit.tp[2])) %>% 
  ggplot() + 
  geom_histogram(
    data = catalog, 
    aes(x = tp_hrs, y = ..density.., fill = 'Observed Historic Catalog'),
    color = 'black', size = 0.25) + 
  geom_line(aes(x = dx, y = fit, size = 'Lognormal Distribution')) + 
  geom_vline(xintercept = casestudy$tp_hrs, linetype = 'dashed') + 
  annotate(geom = 'text', 
    x = casestudy$tp_hrs - 1.8, y = 0.0425, angle = 90,
    label = '2019 Observed', family = 'Segoe UI', size = 8/.pt) + 
  scale_fill_manual(values = 'grey90') + 
  scale_size_manual(values = 0.5) + 
  scale_x_origin('Time to Peak Streamflow @ USGS 11463500 (h)') + 
  scale_y_origin('Probability of Occurrence') + 
  theme(legend.title = element_blank())
if (!publish) g6

```

```{r include = FALSE}
## find percentile of 2019 event
plnorm(casestudy$tp_hrs, meanlog = fit.tp[1], sdlog = fit.tp[2])

```

## $m$

A value of $m=4$ was chosen based on an inspection of observed hydrographs for the Russian River in Sonoma County and on the recommendation found in the National Engineering Handbook.

## Plot real vs. simulated streamflow hydrograph

After validating the estimation of $Q_p$, $t_p$, and $m$, we now assess how well the streamflow component model can replicate the 2019 case study event.
We first generate simulated realizations of streamflow given the real precipitation, then compare the realizations to the true observed hydrograph.

### Load observed streamflow hydrographs for the 2019 event

We start by finding the true streamflow hydrograph for the 2019 event for comparison. 
For this case study, streamflow was measured at USGS gage 11463500, which is at the inlet to the study area.  

```{r}
## define parameters & statistics to request from USGS streamgage service
param <- c('00060', '00065')
names(param) <- c('discharge_cfs', 'gageht_ft')
statcode <- c('00001', '00002', '00003', '00008')
names(statcode) <- c('max', 'min', 'mean', 'median')

## download real gauge information
flow <- readNWISdata(
  sites = 11463500, parameterCd = param, 
  startDate = ymd(casestudy$start_day) - days(1), 
  endDate = ymd(casestudy$end_day) + days(2), 
  service = 'iv', tz = 'America/Los_Angeles') %>% 
  renameNWISColumns

```

### Generate simulated realizations of streamflow parameters

We then generate 1,000 realizations of peak streamflow $Q_p$ and time to peak streamflow $t_p$. 
The hydrograph shape parameter $m$ is held constant. 

```{r}
## load observed precipitation for the 2019 event
precip <- casestudy %>%
  transmute(n.AR = 1, n.precip = 1, IVT_max, duration, precip_mm)

## generate probabilistic realizations of HC (soil moisture) for the 2019 event
precip <- generate_soilmoisture(precip, fit.sm, probabilistic = TRUE, n.hc = 1e3)

## generate probabilistic realizations of R (runoff) for the 2019 event
runoff <- 
  fit_generate_runoff(
    precip = precip, 
    catalog = catalog, 
    probabilistic = TRUE, 
    n.runoff = 1)

## generate probabilistic realizations of Q (streamflow) for the 2019 event 
hydrograph <-
  generate_hydrograph(
    runoff = runoff,
    model.Qp = model.Qp, se.Qp = se.Qp, fit.tp = fit.tp, 
    probabilistic = TRUE,
    n.hydro = 1)

```

### Generate simulated hydrographs for the 2019 event

Finally, we use our simulated parameters to construct synthetic hydrographs for the 2019 event. 
These hydrographs are aggregated and the overall statistics of the timeseries distribution are shown in Figure 5(b).

```{r}
## set constants
baseflow <- 3
simlength <- 10 * 24 * 3600
t <- seq(0, simlength, 360)
m <- 4

## create synthetic streamflow timeseries records
num_cores <- 5
cl <- parallel::makeCluster(num_cores)
registerDoSNOW(cl)
flow.sim <- 
  foreach (i = 1:nrow(hydrograph), 
    .packages = 'lubridate') %dorng% {
    Qp <- hydrograph$Qp_m3s[i]
    tp <- hydrograph$tp_hrs[i]*60^2 #seconds
    q <- apply(cbind(exp(m*(1-t/tp)) * (t/tp)^m * Qp, rep(baseflow, length(t))), 1, max)
    sim <- data.frame(t = lubridate::now() + seconds(t), q = q)
    dt <- sim[which.max(sim$q), 't'] - flow[which.max(flow$Flow_Inst), 'dateTime']
    sim$t <- sim$t - dt
    sim
  } %>% reduce(full_join, by = 't')
stopCluster(cl)
flow.sim[is.na(flow.sim)] <- baseflow

## calculate statistics of synthetic streamflow timeseries records
sequence <- flow.sim$t
flow.matrix <- flow.sim %>% select(-t) %>% as.matrix %>% unname
flow.df <- data.frame(t = sequence) %>% 
  mutate(min = apply(flow.matrix, 1, Min), 
         q05 = apply(flow.matrix, 1, function(x) quantile(x, 0.05, na.rm = TRUE)),
         q25 = apply(flow.matrix, 1, function(x) quantile(x, 0.25, na.rm = TRUE)),
         med = apply(flow.matrix, 1, function(x) median(x, na.rm = TRUE)),
         mean = apply(flow.matrix, 1, Mean), 
         q75 = apply(flow.matrix, 1, function(x) quantile(x, 0.75, na.rm = TRUE)),
         q95 = apply(flow.matrix, 1, function(x) quantile(x, 0.95, na.rm = TRUE)), 
         max = apply(flow.matrix, 1, Max)) %>% 
  mutate(min = ifelse(is.infinite(min), NA, min),
         mean = ifelse(is.nan(mean), NA, mean), 
         max = ifelse(is.infinite(max), NA, max))

## plot panel 5(b): observed vs. simulated hydrograph for the 2019 event
```

```{r fig5b, echo = FALSE}
g8 <- ggplot(flow.df) + 
  geom_ribbon(aes(x = t, ymin = q05, ymax = q95, fill = '90th p.')) + 
  geom_ribbon(aes(x = t, ymin = q25, ymax = q75, fill = '50th p.')) +
  geom_line(aes(x = t, y = med, fill = 'Median'), color = 'grey25') + 
  scale_fill_manual('Simulated \nStreamflow',
    breaks = c('Median', '50th p.', '90th p.'),
    labels = c('Median', '50% P.I.', '90% P.I.'),
    values = c('grey25', 'grey70', 'grey90')) + 
  geom_line(data = flow, 
    aes(x = ymd_hms(dateTime, tz = 'America/Los_Angeles'), y = Flow_Inst/mft^3, 
        color = '2019 Event'), size = 0.75, linetype = 'dashed') + 
  scale_color_manual('Observed \nStreamflow', values = 'black') + 
  scale_y_origin(expression(paste('Streamflow (', m^{3}, s^{-1}, ')')), labels = comma) + 
  scale_x_datetime('Date',
    limits = c(ymd_hms('2019-02-24 12:00:00PM', tz = 'America/Los_Angeles'),
               ymd_hms('2019-03-02 12:00:00AM', tz = 'America/Los_Angeles')), 
    date_breaks = 'day', date_labels = '%b %d', expand = c(0,0)) + 
  theme(legend.position = c(0.9,0.65), 
        legend.margin = margin(-1, 0, -1, 0), 
        plot.margin = margin(10,25,10,10))
if (!publish) g8 

```

```{r fig5, include = publish, echo = FALSE}
## generate figure 5
plot_grid(
  g7, g8, nrow = 2, align = 'v',
  labels = c('(a)', '(b)'), label_fontfamily = 'Segoe UI', label_size = 12,
  label_x = 0.2, label_y = 0.95,
  rel_heights = c(5,4))
ggsave('_figures/fig05_hydrograph.png', width = 8.3, height = 12, units = 'cm')

```

```{r fig6, include = publish, echo = FALSE}
## generate figure 6
plot_grid(
  g5 + theme(legend.position = 'none'), g6 + theme(legend.position = 'none'),
  align = 'v', nrow = 2,
  labels = c('(a)', '(b)'), label_fontfamily = 'Segoe UI', label_size = 12,
  label_x = 0.15, label_y = 0.95) %>% 
  plot_grid(get_legend(g5), nrow = 2, rel_heights = c(12,1))
ggsave('_figures/fig06_lognormal.png', width = 8.3, height = 10, units = 'cm')

```


# $f(INUN|Q)$

This component model estimates inundation ($INUN$) as a function of the streamflow hydrograph ($Q$).
This step includes a large increase in memory and computing requirements compared to the other component models, because each streamflow hydrograph (represented by three parameters) is transformed into inundation heights at thousands of building locations within the study area.

The model used in this implementation is also much more complex than any of the other component models thus far.
We used the hydrodynamic solver LISFLOOD as the base model to estimate inundation heights as a function of streamflow hydrographs. 
The fit of the LISFLOOD model is explained in the `5a_run_lisflood` and `5b_run_bestfit` folders.
We then applied a low-dimensional surrogate model to rapidly estimate inundation based on a pre-computed bank of LISFLOOD inundation maps, as explained in the `5d_populate_grid` and `5e_fit_surrogate` folders.
We refer the user to these locations for more information about the fit and validation of the inundation component model.
Here we focus on the results of the 2019 event case study, which are found in the `5c_run_2019event` folder.

For the 2019 event, we looked at prediction accuracy in three ways. 
The first approach was to assess how well we were able to reproduce downstream hydrographs. 
This is a "channel-focused" method that looks at the amount of water in the river at various points in time and space.
The second approach is a "floodplain-focused" method that compares the number of dry vs. wet cells between the observed and simulated inundation maps. 
The third and final approach is an "impact-focused" method that compared the number of inundated buildings. 
Each of these approaches is explained in more detail below.

## Validate river channel 

### Load observed downstream hydrograph data

We first identify the gages that are downstream of the study area inlet and that have available either stage or discharge data for the 2019 storm.
The locations of these gages are shown in the Figure 7(a) below.
ID labels for each gage were added with the image editing software Inkscape.
We then load the 2019 event hydrographs at each of these gages to use as the observed case in our case study comparison.

```{r}
## identify gages of interest based on geography
gages <- whatNWISsites(stateCd = '06', countyCd = '097') %>% 
  filter(grepl('RUSSIAN', station_nm)) %>% 
  filter(str_length(site_no) == 8) %>% 
  filter(site_no != 11463500)

## load observed discharge & stage height data
flow.obs <- readNWISdata(
  sites = gages$site_no, 
  parameterCd = param, 
  startDate = ymd(casestudy$start_day) - days(30), 
  endDate = ymd(casestudy$end_day) + days(7), 
  service = 'iv', tz = 'America/Los_Angeles') %>% 
  renameNWISColumns %>% 
  filter(!is.na(Flow_Inst) | !is.na(GH_Inst))

## filter out gages with no data
gages <- gages %>% 
  filter(site_no %in% unique(flow.obs$site_no)) %>% 
  arrange(site_no)

## plot panel 7(a): map of gages of interest within the study area
```

```{r fig7a, echo = FALSE}
readNWISsite(siteNumbers = c(11463500, gages$site_no[-2])) %>% 
  st_as_sf(coords = c('dec_long_va', 'dec_lat_va'), crs = 4269) %>% 
  st_transform(6417) %>% 
  ggplot() + 
  geom_sf(data = st_union(sonoma), color = 'grey50', fill = 'grey95') + 
  geom_sf(data = aoi, fill = 'grey50', alpha = 0.1, color = 'black') + 
  geom_sf(data = russian %>% st_transform(6417) %>% st_crop(sonoma), 
          color = 'grey30', size = 0.75) + 
  geom_sf(aes(color = factor(site_no)), size = 2) + 
  scale_color_manual('USGS Gauge', guide = FALSE,
                     values = c('black', roma.colors[-3])) + 
  theme_void() +
  theme(text = element_text(family = 'Segoe UI'),
        plot.title = element_text(family = 'Segoe UI Semibold'))
ggsave('_figures/fig07/fig07_map.png', width = 6, height = 8, units = 'cm')

```

### Load simulated downstream hydrograph data

We used the observed streamflow hydrograph at USGS gage 11463500 as input to LISFLOOD to generate simulated hydrographs at each of the gages of interest.
The LISFLOOD model was run using Sherlock, Stanford's high-performance computing cluster.

```{r}
## load simulated stage data
stage <-
  read.table(
    '_scripts/5_INUN/fit_inundation/5c_run_2019event/results/casestudy.stage', skip = 11) %>%
  setNames(c('t', gages$site_no)) %>%
  pivot_longer(cols = -t, names_to = 'site_no', values_to = 'h') %>%
  mutate(dateTime = flow.obs$dateTime[1] + seconds(t)) %>%  # 
  right_join(flow.obs %>% select(dateTime, site_no, GH_Inst), by = c('dateTime', 'site_no')) %>% 
  rename(h.obs = GH_Inst, h.sim = h) %>% 
  mutate(h.obs = h.obs/mft)

```

### Plot observed vs. simulated hydrographs at gages of interest 

We use the stage data output from LISFLOOD, then manually correct the vertical datums of the gages to be in NAVD88 (Geoid 12A), which is the vertical datum of the LISFLOOD elevation file. 
We then plot observed vs. simulated stage height for the duration of the 2019 event, which is Figure 7(b) in the paper.

```{r}
## calculate gage elevation from LISFLOOD DEM
bed <- 
  raster('_scripts/5_INUN/fit_inundation/5c_run_2019event/results/casestudy_SGC_bedZ.asc',
         crs = projection(aoi))
bed[][bed[]>1e9] <- NA
gages.elev <- gages %>%
  st_as_sf(coords = c('dec_long_va', 'dec_lat_va'), crs = 4269) %>%
  st_transform(6417) %>%
  mutate(lisflood_m = raster::extract(bed, ., small = TRUE)) %>%
  mutate(site_no = toNumber(site_no)) %>% 
  left_join(readNWISsite(gages$site_no) %>% select(site_no, alt_va, alt_datum_cd),
            by = 'site_no')

## get the vertical datum conversion from USGS -> LISFLOOD
## (LISFLOOD/SonomaVegMap is in NAVD88 (Geoid 12A), USGS varies)
gages.elev <- gages.elev %>%
  mutate(datum_conversion_m = c(0, NA, 0.873, 0.854, 0.863)) %>%
  mutate(usgs_m = alt_va/mft + datum_conversion_m)

## plot panel 7(b): timeseries of observed vs. simulated stage height at gages of interest
```

```{r fig7b, echo = FALSE}
stage.plot <- stage %>% 
  mutate(site_no = toNumber(site_no)) %>% 
  filter(site_no != 11463980) %>% 
  left_join(gages.elev, by = 'site_no') %>% 
  mutate(h.obs = h.obs+usgs_m-lisflood_m) %>% 
  filter(dateTime >= ymd_hms('2019-02-23 12:00:00AM') & 
           dateTime <= ymd_hms('2019-03-04 12:00:00AM')) 
offset <- 0 #mean(stage.plot$h.obs) - mean(stage.plot$h.sim)

ggplot(stage.plot) + 
  geom_rect(aes(
    xmin = ymd_hms('2019-02-25 12:00:00AM'), 
    xmax = ymd_hms('2019-02-28 12:00:00AM'),
    ymin = 0, ymax = max(c(max(h.obs), Max(h.sim+offset)))*1.05),
    fill = 'grey90', alpha = 0.25) + 
  geom_line(
    aes(x = dateTime, y = h.obs, linetype = 'Observed', 
        color = factor(site_no)), size = 0.5) +
  geom_line(
    aes(x = dateTime, y = h.sim+offset, linetype = 'Simulated', 
        color = factor(site_no)), size = 0.5) + 
  facet_wrap(~site_no, nrow = 4,
    labeller = labeller(site_no = function(x) paste('USGS', x))) + 
  scale_linetype_manual('Data Type', values = c(2,1)) + 
  scale_color_manual('USGS Gauge', guide = FALSE, values = roma.colors[-3]) + 
  scale_x_datetime('Date', labels = function(z) gsub("^0", "", strftime(z, "%m/%d")),
    date_breaks = 'day', minor_breaks = 'day') +
  scale_y_origin('Water Surface (m)') + 
  theme_bw_custom() + 
  theme(strip.background = element_blank(),
        strip.text = element_text(color = 'black', size = 8),
        legend.position = 'bottom',
        legend.margin = margin(-5, 0, 0, 0),
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
ggsave('_figures/fig07/fig07_timeseries.png', width = 6, height = 12, units = 'cm')

```

## Validate floodplain

For the 2019 event, the "observed" data is not the exact inundation recorded due to this AR, but instead it is the closest-matching inundation map from a series of detailed inundation predictions released by the Sonoma County Permit and Resource Management Department (referred to as the Sonoma GIS map).
The simulated data is the inundation map in the `5c_run_2019event` folder.

### Load "observed" and simulated inundation maps

```{r}
## load "observed" inundation map
source('_data/flood_sonoma/flood_sonoma.R')
flood.sonoma <- flood_sonoma(45)*0.64 + flood_sonoma(46)*0.36

## load simulated inundation map
lisflood <- 
  raster('_scripts/5_INUN/fit_inundation/5c_run_2019event/results/casestudy.max',
         crs = projection(aoi)) %>% 
  overlay(dem.hydro, fun = function(x,y) ifelse(y < 1, NA, x))
lisflood.df <- lisflood %>% raster.df %>% filter(value > 0)

## match raster extents
flood.df <- flood.sonoma %>%
  projectRaster(lisflood) %>%
  raster.df %>% filter(value > 0)

```

### Plot "observed" vs. simulated inundation maps

We compare these two maps on a binary basis and calculate how often our simulated map is able to correctly reproduce wet vs. dry cells compared to the Sonoma GIS "observed" map.
Figure 7(c) below shows the spatial distribution of wet vs. dry accuracy within the extent of the Sonoma GIS model, which is indicated by the dotted line. 

```{r}
## define the spatial extent of the Laguna de Santa Rosa 
## (not included in the Sonoma GIS inundation map --> remove from validation statistics)
laguna <- aoi %>%
  st_cast('POINT') %>% st_coordinates %>%
  .[1:3,] %>%
  rbind(c(1935000, 598500)) %>%
  rbind(c(1930000, 598500)) %>%
  rbind(c(1924500, 593000)) %>%
  rbind(c(1924500, 579500)) %>%
  rbind(.[1,]) %>%
  data.frame %>%
  st_as_sf(coords = c('X', 'Y'), crs = st_crs(6417)) %>%
  st_combine %>%
  st_cast('POLYGON')
laguna <- flood.sonoma %>%
  extent %>% as('SpatialPolygons') %>% st_as_sf %>%
  st_set_crs(crs(flood.sonoma)) %>% st_transform(6417) %>%
  st_crop(laguna, .)
laguna.raster <- laguna %>% as('Spatial') %>% rasterize(lisflood)

## plot panel 7(c): map of inundation prediction accuracy
```

```{r fig7c, echo = FALSE}
flood.map <- flood.sonoma %>%
  projectRaster(lisflood) %>%
  overlay(lisflood, fun = function(x,y) {
    ifelse(x > 0 & y > 0, 0, ifelse (x > 0 & y <= 0, -1, ifelse(x <= 0 & y > 0, 1, NA)))})
ggplot() + 
  geom_sf(data = sonoma %>% st_union %>% st_crop(aoi),
    color = 'grey50', fill = 'grey95') + 
  geom_sf(data = aoi, color = 'black', fill = NA) + 
  geom_raster(data = lisflood.df, aes(x=x, y=y), fill = 'grey70', alpha = 0.5) + 
  geom_raster(
    data = flood.map %>% mask(laguna.raster) %>% 
      raster.df %>% filter(!is.na(value)),
    aes(x=x, y=y, fill = factor(value))) +
  geom_sf(data = laguna, fill = NA, color = 'black', linetype  = 'dotted') +
  scale_fill_manual(
    'Legend', values = c(roma.colors[4], 'black', roma.colors[2]),
    labels = c('False \nNegative', 'Correct', 'False \nPositive')) +
  annotation_scale(
    width_hint = 0.2, height = unit(0.25, 'cm'), text_cex = 2/3, 
    location = 'tl', pad_x = unit(0.5, 'cm'), pad_y = unit(0.1, 'cm')) +
  scale_y_continuous(expand = expansion(mult = c(0,0))) +
  theme_void() +
  theme(text = element_text(family = 'Segoe UI', size = 8),
        legend.position = 'bottom',
        legend.text = element_text(margin = margin(r = 10)),
        legend.margin = margin(0.1, 0.1, 0.1, 0.1, unit = 'cm'),
        legend.title = element_blank())
ggsave('_figures/fig07/fig07_floodmap.png', width = 6, height = 9, units = 'cm')

```

### Calculate accuracy of "observed" vs. simulated wet/dry prediction

We chose three wet vs. dry metrics: hit rate (which penalizes false negatives), false alarm ratio (which penalizes false positives), and the critical success index (which balances over- vs. under-prediction). 
These metrics were used by Wing et al. (2017) and First Street Foundation for fitting a nationwide flood inundation model.
More information on the metrics can be found in the `5a_run_bestfit` folder.

```{r}
## calculate accuracy statistics based on confusion matrix
confusion <- function(x,y) {
  ifelse(x > 0 & y > 0, 0, ifelse (x > 0 & y <= 0, -1, ifelse(x <= 0 & y > 0, 1, NA))) 
}
temp <- flood.sonoma %>% 
  projectRaster(lisflood) %>% 
  overlay(lisflood, fun = function(x,y) confusion(x,y)) %>% 
  overlay(dem, fun = function(x,y) ifelse(y>1, x, NA)) 
temp.laguna <- temp %>% mask(laguna.raster)
tb <- table(temp.laguna[])
hitrate = tb[2] / sum(tb[1:2])
falsealarm = tb[3] / sum(tb[2:3])
fstat = tb[2] / sum(tb)

## report accuracy statistics
```

```{r echo = FALSE}
data.frame(
  'hitrate' = c(unname(hitrate), 1),
  'falsealarm' = c(unname(falsealarm), 0),
  'fstat' = c(unname(fstat), 1)) %>% 
  mutate(temp = c('LISFLOOD', 'Best-Case')) %>% 
  # t %>% as.data.frame %>% 
  gt %>% 
  cols_move_to_start(temp) %>% 
  fmt_percent(c('hitrate', 'falsealarm', 'fstat'), decimals = 1) %>% 
  cols_label(hitrate = 'Hit Rate', falsealarm = 'False Alarm', 
             fstat = 'Critical Success Ratio', temp = '') %>% 
  tab_header(title = 'Floodplain Inundation Accuracy Statistics') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')
  
```

## Validate number of inundated buildings

The accuracy statistics presented above consider the entire map with equal importance. 
However, because the density of residential housing is not constant over the study area, these inundation depths do not have equal impacts.
Therefore we also consider the total number of inundated buildings as a final validation metric.

Here we compare the number of inundated buildings in the "observed" vs. simulated case, but we also have one additional data point to consider.
We described earlier how the "observed" (Sonoma GIS) inundation map is not directly tied to the flooding experienced during the 2019 event. 
News articles following the 2019 event estimated that about 1,900 homes saw at least some level of nonzero inundation, which is information that is directly related to the case study event under consideration. 
We compare that number to the estimated number of inundated buildings both in the "observed" map and in the simulated map, as seen in the table below.

```{r echo = FALSE}
## check number of inundated buildings
buildings.coord <- buildings %>% st_coordinates

## report number of inundation buildings
c(simulated = Sum(terra::extract(rast(flood.map), buildings.coord) >= 0),
  sonoma_gis = Sum(terra::extract(rast(flood.map), buildings.coord) <= 0),
  reported = 1900) %>% 
  t %>% as.data.frame %>% 
  gt %>% 
  fmt_number(c('simulated', 'sonoma_gis', 'reported'), decimals = 0) %>% 
  cols_label(
    simulated = 'LISFLOOD Simulation',
    sonoma_gis = '"Observed" (Sonoma GIS)',
    reported = '2019 Reported Estimate') %>% 
  tab_header(title = 'Number of Inundated Buildings') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

```{r include = FALSE}
## compare "observed" vs. simulated building inundation heights 

# flood.bldg <- cbind(
#   rast(lisflood) %>% terra::extract(buildings.coord),
#   rast(flood.sonoma %>% projectRaster(lisflood)) %>% terra::extract(buildings.coord)) %>% 
#   setNames(c('sim', 'obs')) %>% 
#   mutate(resid = sim-obs/mft) %>% 
#   cbind(buildings.coord) %>% 
#   filter(!is.na(resid)) %>% 
#   filter(sim>0 | obs>0) %>% 
#   arrange(abs(resid)) %>% 
#   st_as_sf(coords = c('X', 'Y'), crs = 6417)
# flood.bldg <- flood.bldg %>% 
#   mutate(group = case_when(
#     abs(resid) > 15 ~ 'big', 
#     abs(resid) > 5 ~ 'med', 
#     sim == 0 & obs == 0 ~ 'zero',
#     TRUE ~ 'small'))
# ggplot(flood.bldg) +
#   geom_point(aes(x = obs/mft, y = sim)) +
#   scale_x_origin('Sonoma GIS Inundation (m)') + scale_y_origin('LISFLOOD Inundation (m)') +
#   geom_parity() + coord_fixed(clip = 'off')

```

```{r include = FALSE}
## repeat the above plot, but with the surrogate model

# ## get simulated inundation
# source('_scripts/INUN_sherlock.R')
# hydrograph <- casestudy %>% mutate(n.AR = 1, n.precip = 1, n.hydro = 1)
# load('_sensitivity/surrogate/checkpoints/samples.Rdata')
# cl <- parallel::makeCluster(num_cores)
# registerDoSNOW(cl)
# inundation <-
#   generate_inundation(
#     hydrograph = hydrograph,
#     samples = samples,
#     buildings = buildings,
#     probabilistic = TRUE, n.inun = 1e3
#   )
# stopCluster(cl)
# 
# ## get "observed" inundation
# buildings$inun <- unlist(terra::extract(rast(flood.sonoma %>% projectRaster(dem)), buildings.coord))
# 
# ## plot "observed" vs. surrogate model inundation heights
# temp <- inundation %>% do.call(cbind, .)
# data.frame(
#   id = attr(inundation, 'wet.bldg'),
#   sim.05 = apply(temp, 1, function(x) quantile(x, 0.05)),
#   sim.25 = apply(temp, 1, function(x) quantile(x, 0.25)),
#   sim.med = apply(temp, 1, median),
#   sim.75 = apply(temp, 1, function(x) quantile(x, 0.75)),
#   sim.95 = apply(temp, 1, function(x) quantile(x, 0.95))) %>%
#   full_join(buildings %>% st_drop_geometry %>%
#               transmute(id = bldg, obs = inun/mft), by = 'id') %>%
#   ggplot() +
#   geom_segment(aes(x = obs, xend = obs, y = sim.05, yend = sim.95), color = 'grey70') +
#   geom_point(aes(x = obs, y = sim.med)) +
#   scale_x_origin('Sonoma GIS Inundation (m)') + scale_y_origin('Surrogate Model Inundation (m)') +
#   geom_parity() + coord_fixed(clip = 'off')
  
```


# $f(DM|INUN)$

The next component model in the framework is the estimation of building damage ($DM$) as a function of first floor water level.
We have implemented this component model by using depth-damage curves. 
Because of the significant uncertainty in predicting damage as a function of flood depth alone, we use two different depth-damage relationships: one from Hazus-MH and one from Wing et al. (2020).

## Select depth-damage relationships

Hazus-MH uses deterministic, monotonic depth-damage relationships. 
Curves are chosen based on (a) the number of stories and (b) whether or not there is a basement present. 
On the other hand, the Wing et al. (2020) depth-damage relationships only consider depth, but are probabilistic functions represented by the beta distribution. 
A visual comparison of the two curves at different flood heights is included in Figure 8(a) in the paper. 
We refer the user to the paper for a discussion of the merits of each one.

```{r}
## plot panel 8(a): comparison of HAZUS vs. Wing et al. (2020) distributions
```

```{r fig8a, echo = FALSE}
## generate Wing et al. (2020) distributions
dx <- 0.01
beta.dist <- 
  map_dfc(.x = 1:nrow(wing2020), 
    .f = ~dbeta(x = seq(dx, 1-dx, dx), shape1 = wing2020$alpha[.x], 
                shape2 = wing2020$beta[.x])) %>%
    setNames(wing2020$depth_ft) %>% 
    mutate(damage_pct = seq(dx, 1-dx, dx)) %>% 
    pivot_longer(cols = -damage_pct, names_to = 'depth_ft', values_to = 'damage_prob') %>% 
    mutate(depth_ft = toNumber(depth_ft)) 

g13 <- ggplot() + 
  ggridges::geom_density_ridges(
    data = beta.dist,
    aes(x = damage_pct, y = depth_ft, group = depth_ft, height = damage_prob,
        fill = 'Wing et al.'), 
    color = 'grey50', alpha = 0.6, stat = 'identity') + 
  geom_line(
    data = wing2020 %>% rbind(rep(0, 5)), 
    aes(x = mu, y = depth_ft), color = 'grey60', size = 0.75) + 
  geom_point(
    data = wing2020 %>% rbind(rep(0, 5)), 
    aes(x = mu, y = depth_ft), color = 'grey60') + 
  geom_line(
    data = hazus %>% filter(class == '1 N Struct'), 
    aes(x = damage_pct, y = depth_ft, color = 'Hazus-MH'), size = 0.75) + 
  scale_color_manual('', values = 'black') + 
  scale_fill_manual('', values = 'grey70') + 
  scale_x_continuous('Damage Ratio', labels = percent, 
                     expand = expansion(mult = c(0,0))) + 
  scale_y_origin('First Floor Water Depth (m)', breaks = seq(0, 5, 0.5)*mft, 
                 labels = comma_format(accuracy = 0.1, scale = 1/mft)) + 
  coord_flip(xlim = c(0,1), clip = 'off') + 
  theme(legend.position = c(0.8, 0.78), 
        legend.margin = margin(-10, 0, -10, 0))
if (!publish) g13 

```

## Load "observed" damage data

We were unable to identify any direct damage data for the 2019 event. 
Instead we are using safety tags from the Sonoma County Rapid Evaluation Safety Assessment (RESA) that were assigned to buildings immediately after the 2019 event. 
The paper discusses some of the implications and limitations of this proxy.
The table below (Table 2 in the paper) summarizes the number of buildings with nonzero first floor water levels, broken out by the status of the RESA tag.

```{r}
## calculate aggregated tag/safety categories
buildings <- buildings %>% 
  mutate(safety = case_when(
    RESA_Status_GIS == 'Green' ~ 'tagged/safe', 
    RESA_Status_GIS == 'Yellow' ~ 'tagged/unsafe',
    RESA_Status_GIS == 'Red' ~ 'tagged/unsafe',
    TRUE ~ 'untagged')) %>% 
  mutate(safety = factor(safety, levels = c('untagged', 'tagged/safe', 'tagged/unsafe')))

## calculate inundation due to the Sonoma GIS map 
buildings$inun <- 
  terra::extract(rast(flood.sonoma %>% projectRaster(dem)), 
                 st_coordinates(buildings)) %>% unlist

## report number & percent of inundated buildings by safety category
```

```{r tab2, echo = FALSE}
buildings %>% 
  st_drop_geometry %>% 
  group_by(safety) %>% 
  summarize(
    `Number of Inundated Buildings` = Sum(inun>0) %>% comma(accuracy = 1),
    `Number of Total Buildings` = length(inun) %>% comma(accuracy = 1),
    `Percent Inundated` = (Sum(inun>0)/length(inun)) %>% percent(accuracy = 0.01)) %>% 
  column_to_rownames('safety') %>%
  t %>% as.data.frame %>%
  rownames_to_column('temp') %>% 
  gt %>% 
  fmt_markdown(temp) %>% 
  cols_label(
    'temp' = '', 
    'untagged' = 'Untagged', 
    'tagged/safe' = 'Tagged/Safe', 
    'tagged/unsafe' = 'Tagged/Unsafe') %>% 
  tab_header(title = 'RESA Safety Category vs. Building Inundation') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

## Plot damages by safety category and by depth-damage relationship

We generated 1,000 realizations of damage for the inundation heights reported by the Sonoma GIS map. 
The results of this process are summarized in Figure 8(b) below. 

```{r results = 'hide'}
## load depth-damage function(s)
source('_scripts/6_DM/DM.R')

## format existing inundation info for f(DM|INUN) function
wet.bldg <- which(buildings$inun > 0)
inundation <- list(matrix(buildings$inun[wet.bldg]))
attributes(inundation)$n.inun <- NA
attributes(inundation)$buildings <-
  st_coordinates(buildings) %>%
  cbind(id = 1:nrow(.), .) %>%
  .[wet.bldg,]
attributes(inundation)$wet.bldg <- wet.bldg

## format depth-damage information
hazus <- hazus %>% 
  filter(Basement == 'Y') %>%
  mutate(depth_m = depth_m + 3/mft) %>%
  group_by(depth_m) %>%
  summarize(damage_min = Min(damage_pct),
            damage_mean = Mean(damage_pct),
            damage_max = Max(damage_pct))
flemo <- flemo %>%
  group_by(depth_m) %>%
  summarize(damage_min = Min(damage_pct),
            damage_mean = Mean(damage_pct),
            damage_max = Max(damage_pct))

## calculate damage using the Wing et al. (2020) relationships
cl <- parallel::makeCluster(num_cores)
registerDoSNOW(cl)
damage.beta <- generate_damage(
  inundation, 
  buildings = buildings %>% st_drop_geometry %>% rename(GEOID = tract),
  foundations = nsi1.base,
  curve = 'beta', 
  hazus = hazus, beta = wing2020,
  probabilistic = TRUE,
  n.damage = 1e3)
damage.beta <- damage.beta[[1]] %>% 
  right_join(buildings %>% st_drop_geometry, ., by = 'bldg')

## calculate damage using the Hazus-MH relationships
damage.hazus <- generate_damage(
  inundation,
  buildings = buildings %>% st_drop_geometry %>% rename(GEOID = tract),
  foundations = nsi1.base,
  curve = 'hazus',
  hazus = hazus, beta = wing2020,
  probabilistic = TRUE,
  n.damage = 1e3)
damage.hazus <- damage.hazus[[1]] %>% 
  right_join(buildings %>% st_drop_geometry, ., by = 'bldg')
stopCluster(cl)

## plot panel 8(b): histograms of damage ratio by safety category 
## and by depth-damage relationship
```

```{r fig8b, echo = FALSE}
## combine damages into one dataframe
num_inun <- buildings %>% 
  st_drop_geometry %>% 
  group_by(safety) %>% 
  summarize(num_inun = Sum(inun>0)) %>% 
  pull(num_inun)
tags <- c(
  paste0('Untagged \n(n = ', num_inun[1], ')'), 
  paste0('Tagged/Safe \n(n = ', num_inun[2], ')'),
  paste0('Tagged/Unsafe \n(n = ', num_inun[3], ')'))
damage <- left_join(
  damage.beta %>% 
    mutate(dm = case_when(is.na(dm) ~ 0, TRUE ~ dm)) %>%
    transmute(bldg, n.damage, dm.beta = dm, safety),
  damage.hazus %>% 
    mutate(dm = case_when(is.na(dm) ~ 0, TRUE ~ dm)) %>%
    transmute(bldg, n.damage, dm.hazus = dm, safety),
  by = c('bldg', 'n.damage', 'safety')) %>% 
  mutate(safety = case_when(
    safety == 'untagged' ~ tags[1],
    safety == 'tagged/safe' ~ tags[2],
    safety == 'tagged/unsafe' ~ tags[3]) %>% 
      factor(levels = tags)) %>% 
  pivot_longer(cols = c(dm.hazus, dm.beta), names_to = 'curve', values_to = 'dm') %>% 
  mutate(curve = case_when(
    curve == 'dm.beta' ~ 'Wing et al.\n', curve == 'dm.hazus' ~ 'Hazus-MH\n'))

g16 <- ggplot(damage) + 
  geom_histogram(
    aes(x = dm, y = ..density.., fill = safety), boundary = 0, bins = 15,
    color = 'black', size = 0.25, show.legend = FALSE) +
  lemon::facet_rep_grid(curve ~ safety, switch = 'y') + 
  scale_fill_manual(breaks = tags, values = roma.colors[3:1]) + 
  scale_x_continuous('Simulated Damage Ratio', 
    limits = c(0,1), expand = expansion(mult = c(0,0)),
    labels = percent_format(accuracy = 1)) + 
  scale_y_origin('Probability of Occurrence \n') + 
  coord_flip(clip = 'off') + 
  theme(strip.background = element_blank(), strip.placement = 'outside',
        panel.spacing.x = unit(0.1, 'lines'), panel.spacing.y = unit(0.1, 'lines'))
if (!publish) g16 

```

```{r fig8, include = publish, echo = FALSE}
plot_grid(
  g13, g16, nrow = 2, 
  labels = c('(a)', '(b)'), label_fontfamily = 'Segoe UI', 
  label_size = 12, label_y = c(0.2, 0.95),
  rel_heights = c(3,5))
ggsave('_figures/fig08_damage.png', width = 8.3, height = 10, units = 'cm')

```

# $f(DV|DM)$

The final component model in the PARRA framework is not directly validated as part of this case study, because we do not have building-level damage data to use as input and we do not have building-level loss data to validate our predictions against. 
Please see the script `lossexceedance.Rmd` for calculations related to the next portion of the paper. 
