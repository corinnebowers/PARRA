---
title: "Sonoma County AR Flood Loss Analysis"
author: "Corinne Bowers"
date: "3/9/2022"
output:
  html_document:
    toc: true 
    toc_float: true
    #toc_depth: 3  
    code_folding: hide
    number_sections: true 
    theme: spacelab   #https://www.datadreaming.org/post/r-markdown-theme-gallery/
    highlight: tango  #https://www.garrickadenbuie.com/blog/pandoc-syntax-highlighting-examples/
---

The purpose of this script is to reproduce figures and numeric results for the paper "A Performance-Based Approach to Quantify Atmospheric River Flood Risk" (https://doi.org/10.5194/nhess-2021-337).
This file focuses on probabilistic results from running the entire PARRA framework in sequence for the lower Russian River in Sonoma County, California.

Please note: all figures are formatted for publication, therefore certain features may not display correctly in this markdown file. 

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = 'D:/1-PARRA/')
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(results = 'hold', fig.show = 'hold', fig.align = 'center')
rm(list=ls())

```

```{r}
## setup information
source('_data/setup.R')
source('_data/plots.R')
xmax <- 350e6

## load required packages
require(units)
require(httr)
require(jsonlite)
require(curl)

## load historic catalog
load('_data/catalog/catalog.Rdata')

## load location information
load('_data/lisflood/dem.Rdata')
load('_data/aoi/aoi.Rdata')
load('_data/NHD/NHD.Rdata')
load('_data/buildings/buildings.Rdata')

## load NFIP claims & policies
load('_data/NFIP/NFIP.Rdata')

## load inflation rate relative to 2019
inflation <- read.csv('_data/NFIP/inflation2019.csv', header = FALSE) %>% 
  setNames(c('year', 'rate'))

```

```{r echo = FALSE}
## should figures be saved out for the publication?
publish <- FALSE

if (!publish) {
  theme_set(
    theme_classic() + theme(
      text = element_text(family = 'Segoe UI', size = 12),
      axis.line = element_line(size = 0.5),
      axis.ticks = element_line(size = 0.5, color = 'black'),
      legend.key.size = unit(0.5, 'cm')))
  }

```


# Scenario Loss Results

We chose two particularly damaging AR events as defined in the `demonstration.Rmd` file to examine as scenarios: the February 2019 event and the January 2017 event.
We started with the real maximum IVT and duration from each event and generated 10,000 probabilistic realizations of loss using Sherlock, Stanford's high-performance computing cluster.
We also include a third hypothetical scenario based on the February 2019 event with perfect information about predicted precipitation, where Sherlock simulations started with the observed precipitation rather than observed AR characteristics.
The loss histograms for all three of these scenarios are included in Figure 9.

All files associated with this calculation are stored in the `_results/2019event/` folder.
The dataframe *loss.sim* within the `DV.Rdata` file contains the probabilistic realizations of expected total loss for each of the 10,000 simulations.

## February 2019 event

```{r}
## load observed loss
loss.est <- 91.6e6

## load simulated loss from PARRA framework 
load('_results/event_feb2019/checkpoints/DV.Rdata')

## plot figure 9(a): observed vs. simulated loss
```

```{r fig9a, echo = FALSE}
g2019 <- loss.sim %>% 
  right_join(data.frame(n.precip = 1:1e4)) %>% 
  mutate(loss = case_when(!is.na(loss) ~ loss, TRUE ~ 0)) %>% 
  ggplot() + 
    geom_histogram(aes(x = loss, y = ..density..), color = 'black', fill = 'grey90', 
      binwidth = 5e6, boundary = 0, size = 0.25) + 
    geom_vline(xintercept = loss.est, linetype = 'dashed') + 
    annotate('text', x = loss.est, y = 2.5e-8, vjust = 1.35, angle = 90,
      label = 'Observed', family = 'Segoe UI', size = 8/.pt) +
    scale_x_origin('Loss Estimate',
      labels = comma_format(scale = 1e-6, accuracy = 1, prefix = '$', suffix = 'M')) + 
    scale_y_origin('Frequency of Occurrence') + 
  coord_cartesian(xlim = c(0, xmax), ylim = c(0, 4.25e-8)) + 
  theme(
    plot.background = element_rect(fill = NA, color = NA), 
    axis.title.y = element_text(color = NA),
    axis.title.x = element_blank())
if(!publish) g2019

```

```{r echo = FALSE}
## calculate values of interest
data.frame(
  event = 1 - (sum(loss.sim$loss > loss.est) / 1e4),
  maxloss = max(loss.sim$loss)) %>% 
  gt %>%
  fmt_percent('event', decimals = 1) %>% 
  fmt_currency('maxloss', suffixing = TRUE, decimals = 0) %>% 
  cols_label(event = 'Percentile of Observed Loss', maxloss = 'Max Simulated Loss') %>% 
  tab_header('PARRA Loss Results for February 2019 Event') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')
  
```

## January 2017 event 

```{r}
## load observed loss
loss.est <- 12.5e6 * inflation$rate[inflation$year==2017]

## load simulated loss from PARRA framework 
load('_results/event_jan2017/checkpoints/DV.Rdata')

## plot figure 9(b): observed vs. simulated loss
```

```{r fig9b, echo = FALSE}
g2017 <- loss.sim %>% 
  right_join(data.frame(n.precip = 1:1e4)) %>% 
  mutate(loss = case_when(!is.na(loss) ~ loss, TRUE ~ 0)) %>% 
  ggplot() + 
    geom_histogram(aes(x = loss, y = ..density..), color = 'black', fill = 'grey90', 
      binwidth = 5e6, boundary = 0, size = 0.25) + 
    geom_vline(xintercept = loss.est, linetype = 'dashed') + 
    annotate('text', x = loss.est, y = 2.5e-8, vjust = 1.35, angle = 90,
      label = 'Observed', family = 'Segoe UI', size = 8/.pt) +
    scale_x_origin('Loss Estimate',
      labels = comma_format(scale = 1e-6, accuracy = 1, prefix = '$', suffix = 'M')) + 
    scale_y_origin('Frequency of Occurrence') + 
  coord_cartesian(xlim = c(0, xmax), ylim = c(0, 4.25e-8)) + 
  theme(axis.title.x = element_blank())
if(!publish) g2017

```

```{r echo = FALSE}
## calculate values of interest
data.frame(
  event = 1 - (sum(loss.sim$loss > loss.est) / 1e4),
  maxloss = max(loss.sim$loss)) %>% 
  gt %>%
  fmt_percent('event', decimals = 1) %>% 
  fmt_currency('maxloss', suffixing = TRUE, decimals = 0) %>% 
  cols_label(event = 'Percentile of Observed Loss', maxloss = 'Max Simulated Loss') %>% 
  tab_header('PARRA Loss Results for January 2017 Event') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')
  
```

## February 2019 event with perfect precipitation information

```{r}
## load observed loss
loss.est <- 91.6e6

## load simulated loss from PARRA framework 
load('_results/event_feb2019_prcpconst/checkpoints/DV.Rdata')

## plot figure 9(c): observed vs. simulated loss
```

```{r fig9c, echo = FALSE}
g2019_obsprcp <-
  ggplot(loss.sim) + 
    geom_histogram(aes(x = loss, y = ..density..), color = 'black', fill = 'grey90', 
      binwidth = 5e6, boundary = 0, size = 0.25) + 
    geom_vline(xintercept = loss.est, linetype = 'dashed') + 
    annotate('text', x = loss.est, y = 2.5e-8, vjust = 1.35, angle = 90,
      label = 'Observed', family = 'Segoe UI', size = 8/.pt) +
    scale_x_origin('Loss Estimate',
      labels = comma_format(scale = 1e-6, accuracy = 1, prefix = '$', suffix = 'M')) + 
    scale_y_origin('Frequency of Occurrence') + 
  coord_cartesian(xlim = c(0, xmax), ylim = c(0, 4.25e-8)) + 
  theme(
    plot.background = element_rect(fill = NA, color = NA), 
    axis.title.y = element_text(color = NA))
if(!publish) g2019_obsprcp

```

```{r echo = FALSE}
## calculate values of interest
data.frame(
  event = 1 - (sum(loss.sim$loss > loss.est) / 1e4),
  maxloss = max(loss.sim$loss)) %>% 
  gt %>%
  fmt_percent('event', decimals = 1) %>% 
  fmt_currency('maxloss', suffixing = TRUE, decimals = 0) %>% 
  cols_label(event = 'Percentile of Observed Loss', maxloss = 'Max Simulated Loss') %>% 
  tab_header(
    'PARRA Loss Results for Hypothetical Event', 
    subtitle = 'February 2019 with Known Precipitation (Perfect Information Case)') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')
  
```

## Combine plots and save as Figure 10
```{r fig9, include = publish, echo = FALSE}
## generate figure 9
plot_grid(
  g2019 + ggtitle(label = waiver(), subtitle = 'February 2019'), 
  g2017 + ggtitle(label = waiver(), subtitle = 'January 2017'), 
  g2019_obsprcp + ggtitle(label = waiver(), subtitle = 'February 2019 - Known Precipitation'), 
  nrow = 3, rel_heights = c(8,8,9),
  labels = c('(a)', '(b)', '(c)'), label_fontfamily = 'Segoe UI', label_size = 12,
  label_x = 0.8, label_y = 0.9)
ggsave('_figures/fig09_lossevents.png', width = 8.3, height = 12, units = 'cm')

```

## Plot spatial distribution of simulated loss

The dataframe *loss.group* contains the average total loss aggregated by some spatial grouping, which in this case is census block groups.
Figure 10 shows the expected spatial distribution of loss for the February 2019 event as predicted by the PARRA framework.

```{r}
## load observed loss
loss.est <- 91.6e6

## load simulated loss for February 2019 from PARRA framework 
load('_results/event_feb2019/checkpoints/DV.Rdata')

## attach loss information to Sonoma County census block groups
sonoma_cbgs <- block_groups(state = 'CA', county = 'Sonoma') %>% 
  mutate(group = toNumber(GEOID)) %>% 
  left_join(loss.group %>% mutate(group = toNumber(group)), by = 'group') %>% 
  mutate(loss = ifelse(is.na(loss), 0, loss)) %>% 
  st_transform(6417) %>% 
  st_crop(aoi)

## save loss information as CSV files for Github
loss.sim %>% 
  right_join(data.frame(n.precip = 1:1e4), by = 'n.precip') %>%
  mutate(loss = case_when(is.na(loss)~0, TRUE~loss)) %>% 
  write.table(file = '_results/event_feb2019/loss_sim.csv', row.names = FALSE, col.names = TRUE, sep = ',')
loss.group %>% 
  rename('censusblockgroup' = group) %>% 
  write.table(file = '_results/event_feb2019/loss_group.csv', row.names = FALSE, col.names = TRUE, sep = ',')

## plot figure 10: spatial distribution of simulated loss
```

```{r fig10, echo = FALSE}
ggplot(sonoma_cbgs) + 
  geom_sf(data = sonoma, fill = 'grey90', color = 'grey60', size = 0.25) +
  geom_sf(aes(fill = loss/1e6), color = NA) + 
  scale_fill_scico('Block Group \nLoss Estimate', palette = 'roma', 
    begin = 0.5, labels = comma_format(prefix = '$', suffix = 'M')) + 
  ggnewscale::new_scale_fill() + 
  geom_sf(aes(fill = loss>0), color = NA, show.legend = FALSE) + 
  scale_fill_manual(values = c('white', NA), na.value = NA) + 
  geom_sf(data = sonoma, fill = NA, size = 0.25, color = 'grey60') +
  geom_sf(data = st_union(sonoma), color = 'grey50', fill = NA) + 
  geom_sf(data = aoi, fill = NA, color = 'grey40') + 
  geom_sf(data = russian %>% st_transform(6417) %>% st_crop(sonoma), size = 0.75) + 
  coord_sf(expand = FALSE) +
  theme(axis.title = element_blank(), axis.text = element_blank(), 
        axis.ticks = element_blank(), axis.line = element_blank(),
        panel.background = element_blank(), plot.background = element_blank(),
        legend.position = c(0.1, 0.275), legend.background = element_blank())
ggsave('_figures/fig10_lossmap.png', width = 8.3, height = 6, units = 'cm')

```


# Average Annual Loss (AAL)

We move now from examining scenario events to incorporating the full historic catalog into our analysis.
This captures the full character of AR-induced fluvial flood risk in the lower Russian River study area.
We generate 100 realizations of loss for each of the 382 events in the historic catalog. 
Because the catalog spans 32 years, this creates a synthetic record representing $32 \times 100 = 3200$ years of AR events within the study area. 
We refer the user to the paper for a more in-depth discussion of the generation of this stochastic record.
We summarize the results of these simulations using the annual average loss (AAL), a common metric for risk assessment. 

## Calculate AAL based on PARRA framework

We calculate the AAL from the PARRA framework based on Equation 6 from the paper and repeat the calculate 1000 times to estimate the mean and standard deviation.
The loss results from the stochastic record are stored in the `_results/lossexceedance/` folder.

```{r}
## load loss realizations for stochastic record 
files <- list.files('_results/stochastic/checkpoints', full.names = TRUE)

## calculate AAL for each of the Monte Carlo runs
AAL_uncertainty <- 
  foreach(file = files, .combine = 'c') %do% {
    load(file)
    sum(loss.sim$loss)/3200
  }

## calculate quantiles of the simulated AAL distribution
quantile(
  AAL_uncertainty/1e6, 
  c(0.005, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 0.995)) %>% round

## record mean AAL
AAL <- c('PARRA' = mean(AAL_uncertainty))

```

## Calculate AAL based on NFIP claims

The flood AAL for the study area is a number subject to significant uncertainty, so it is difficult to directly validate the AAL estimate from the PARRA framework. 
Instead we provide one additional estimate of the flood AAL in the study area using flood insurance claims from the National Flood Insurance Program (NFIP).

We first find the average annual insured losses from the census tracts within the study area.
We estimate insurance penetration rate by dividing the number of NFIP policies by the number of households, then scale the average annual insured losses by this penetration rate to estimate total AAL for all households. 
The resulting number is reported in 2019 dollars.

```{r}
## find the average annual insured losses from NFIP claims data
aal.claims <- claims %>%
  left_join(inflation, by = c('yearofloss' = 'year')) %>% 
  mutate(
    buildingclaim_2019 = case_when(
      is.na(amountpaidonbuildingclaim) ~ 0, 
      TRUE ~ amountpaidonbuildingclaim * rate),
    contentsclaim_2019 = case_when(
      is.na(amountpaidoncontentsclaim) ~ 0, 
      TRUE ~ amountpaidoncontentsclaim * rate)) %>% 
  right_join(sonoma[aoi,] %>% st_drop_geometry %>% transmute(censustract = toNumber(GEOID))) %>% 
  mutate(wateryear = yearofloss + ifelse(month(dateofloss) %in% 10:12, 1, 0)) %>% 
  group_by(wateryear) %>% 
  summarize(min = min(dateofloss), 
            max = max(dateofloss),
            structureloss = Sum(buildingclaim_2019),
            contentsloss = Sum(contentsclaim_2019),
            totalloss = structureloss + contentsloss) %>% 
  filter(wateryear >= 1978 & wateryear <= 2019) %>% 
  summarize(loss = Sum(totalloss)/nrow(.)) %>% .$loss

## find the number of NFIP policies per year in the study area
insurancepolicies <- policies %>% 
  right_join(sonoma[aoi,] %>% st_drop_geometry %>% transmute(censustract = toNumber(GEOID))) %>% 
  mutate(wateryear = year(policyeffectivedate) + 
           ifelse(month(policyeffectivedate) %in% 10:12, 1, 0)) %>% 
  group_by(wateryear) %>% 
  summarize(min = min(ymd(policyeffectivedate)), 
            max = max(ymd(policyeffectivedate)), 
            n = length(policyeffectivedate)) %>% 
  filter(wateryear >= 2010 & wateryear <= 2019) %>% 
  summarize(avg = mean(n)) %>% .$avg

## find the number of residences per year in the study area
profile_vars <- listCensusMetadata(name = 'acs/acs5/profile', vintage = 2018)
housingunits <-
  getCensus(name = 'acs/acs5/profile', vars = 'group(DP04)', vintage = 2018,
            regionin = 'state:06+county:097', region = 'tract:*') %>% 
  select(-state, -GEO_ID, -NAME) %>% 
  select(-ends_with('M'), -ends_with('A'), -ends_with('PE')) %>% 
  pivot_longer(cols = ends_with('E'), names_to = 'variable', values_to = 'estimate') %>% 
  left_join(profile_vars %>% select('name', 'label'), by = c('variable' = 'name')) %>% 
  filter(variable == 'DP04_0001E') %>% 
  mutate(censustract = 6*1e9 + toNumber(county)*1e6 + toNumber(tract)) %>% 
  right_join(sonoma[aoi,] %>% st_drop_geometry %>% transmute(censustract = toNumber(GEOID))) %>% 
  pull(estimate) %>% Sum

## find the insurance penetration rate
penetration <- insurancepolicies/housingunits

## find the average annual loss
AAL <- c(AAL, 'NFIP' = aal.claims/penetration)

```

```{r echo = FALSE}
AAL %>% 
  data.frame %>% 
  rownames_to_column %>% 
  setNames(c('source', 'value')) %>% 
  gt %>%
  fmt_markdown('source') %>% 
  fmt_currency('value', suffixing = TRUE, decimals = 0) %>% 
  cols_label(source = 'Source', value = 'Estimate') %>% 
  tab_header('AR Flood AAL', subtitle = 'Lower Russian River, Sonoma County, CA') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

These two numbers generally agree in terms of their order of magnitude, therefore our estimate is well within the tolerable range of uncertainty.


# Loss Exceedance Curve

The next loss metric to consider for the historic catalog is the loss exceedance curve, which plots a range of loss values vs. their expected rate of occurrence $\lambda$. 
The loss exceedance is characterized theoretically in Equation 1 of the paper and approximated numerically in Equation 7.

## Plot the loss exceedance curve

```{r}
## load mean simulation for stochastic record 
data.frame(file = files, aal = AAL_uncertainty) %>% 
  arrange(aal) %>% 
  filter(aal > AAL['PARRA']) %>% .[1,] %>% 
  pull(file) %>% load
loss.stochastic <- loss.sim

## estimate occurrence rates for every loss realization
lossexceedance <- loss.stochastic %>% 
  right_join(expand.grid(n.AR = 1:nrow(catalog), n.precip = 1:100)) %>%
  mutate(loss = case_when(!is.na(loss) ~ loss, TRUE ~ 0)) %>%
  transmute(rp = (nrow(.):1)/3200, stochastic = sort(loss))

## plot the loss exceedance curve
```

```{r echo = FALSE}
ggplot(lossexceedance) + 
  geom_step(aes(x = stochastic, y = rp), size = 1) + 
  geom_hline(aes(yintercept = 1e-2, color = '100 Year Event'), linetype = 'dashed') + 
  geom_point(aes(x = stochastic[rp<1e-2][1], y = 1e-2, color = '100 Year Event'), size = 3) +
  geom_vline(aes(xintercept = loss.est, color = '2019 Event')) + 
  geom_point(aes(x = loss.est, y = rp[stochastic>loss.est][1], color = '2019 Event'), size = 3) +
  scale_color_manual('Points of Interest', values = c('grey40', 'grey70')) + 
  scale_x_origin('Loss ($M)', labels = comma_format(scale = 1e-6), breaks = seq(0, 5e8, 5e7)) + 
  scale_y_log10('Annual Rate of Occurrence, \u03bb', labels = scientific) +
  annotation_logticks(sides = 'l') +
  theme(legend.position = c(0.8,0.8))

```

## Examine points of interest on the loss exceedance curve 

Based on the curve above, we can examine loss events in two ways: we can choose a specific loss threshold and estimate the occurrence rate of an event of that magnitude (the vertical line), or we can choose a specific occurrence rate and estimate what loss magnitude is expected for that value (the horizontal line). 
We provide calculation examples for both.

### Estimate return period for the 2019 event

We start by looking vertically: choosing a loss threshold and estimating rate of occurrence.
One particular loss event of interest is the 2019 case study event, which caused \$91.6 million dollars of damage. 
We indicate this event with the solid vertical line on the figure above.
We can see that this event is expected to occur more frequently than the 100-year return period. 
The exact rate of occurrence and estimated return period are as follows:

```{r echo = FALSE}
## calculate rate of occurrence for 2019 event
p <- sum(loss.stochastic$loss > 91.6e6)/3200

## show values as formatted table
data.frame(
  label = c('Annual Rate of Occurrence', 'Estimated Return Period'),
  value = c(p, round(1/p, 2))) %>% 
  gt %>% 
  fmt_markdown('label') %>% 
  fmt_number('value', n_sigfig = 4, drop_trailing_zeros = TRUE) %>% 
  cols_label(label = 'Parameter', value = 'Value') %>% 
  tab_header('Frequency of 2019 Event in Stochastic Record') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

### Estimate expected loss associated with the 100-year event

Next we look horizontally: choosing a rate of occurrence and estimating the expected loss with that rate of occurrence.
We examine the 100 year event $(\lambda = 0.01)$ because this is a common choice for risk management and mitigation planning.
This is shown as the dashed horizontal line. 
The loss estimate associated with the 100 year event is: 

```{r echo = FALSE}
## calculate expected loss associated with the 100 year event
loss.stochastic %>% 
  arrange(desc(loss)) %>% 
  mutate(p = (1:nrow(.))/3200) %>% 
  mutate(RP = 1/p) %>% 
  filter(p == 0.01) %>%
  mutate(loss = loss/1e6) %>% pull(loss) %>% 
  comma(prefix = '$', suffix = 'M', accuracy = 0.01)

```


# Hypothetical Mitigation Action

We considered home elevations as a potential mitigation action.
Households in the study area were raised above the 100-year floodplain one-by-one, with priority assigned based on their proximity to the Russian River.
We iteratively recalculated the AAL until it fell below the target threshold of 50% of the original AAL.
Calculations were performed on Sherlock, Stanford's high-performance computing cluster, and results are stored in the `_results/mitigation` folder.

## Calculate Sonoma's mitigation investment through FEMA HMAP (hazard mitigation assistance projects)
```{r}
## get number of data points from FEMA API
h <- handle_setopt(new_handle())
api_call <- paste0(
  'https://www.fema.gov/api/open/v2/HazardMitigationAssistanceProjects?',
  '$inlinecount=allpages&', '$top=1&',
  '$filter=state%20eq%20%27California%27&')
api <- curl_download(api_call, tempfile(), handle = h)
n <- fromJSON(api)$metadata$count  #should be around 2,000

## get HMA projects dataset from FEMA API
hmap <- 
  foreach(
    i = 0:(n/1000), .combine = 'rbind', 
    .packages = c('httr', 'curl', 'jsonlite')) %do% {
      h <- handle_setopt(new_handle())
      api_call <- paste0(
        'https://www.fema.gov/api/open/v2/HazardMitigationAssistanceProjects?',
        '$skip=', i*1000, '&$filter=state%20eq%20%27California%27')
      api <- curl_download(api_call, tempfile(), handle = h)
      fromJSON(api)$HazardMitigationAssistanceProjects
    }

## filter to relevant
hmap <- hmap %>% 
  filter(county == 'Sonoma') %>% 
  filter(!grepl('seismic', str_to_lower(projectType))) %>% 
  filter(!grepl('wildfire', str_to_lower(projectType))) %>%
  filter(grepl('elevation', str_to_lower(projectType)) |
           grepl('acquisition', str_to_lower(projectType))) %>% 
  filter(!grepl('public', str_to_lower(projectType)))
# summary(hmap$programFy)

## calculate amount spent on elevations and acquisitions
hmap %>%
  left_join(inflation, by = c('programFy' = 'year')) %>%
  mutate(costShareAmount = projectAmount * costSharePercentage) %>%
  summarize(projectAmount = sum(projectAmount*rate)/1e6,
            numberOfProperties = sum(numberOfProperties),
            federalShareObligated = sum(federalShareObligated*rate)/1e6,
            costShareAmount = sum(costShareAmount*rate)/1e6) %>% 
  transmute(`Number of Properties` = numberOfProperties, 
            `Total Investment ($M)` = projectAmount, 
            `Federal Obligation ($M)` = federalShareObligated) %>% 
  t %>% as.data.frame %>% rownames_to_column %>% 
  setNames(c('label', 'value')) %>% 
  gt %>% 
  fmt_markdown('label') %>% 
  fmt_number('value', decimals = 2, drop_trailing_zeros = TRUE) %>% 
  cols_label(label = 'Parameter', value = 'Value') %>%
  tab_header('FEMA HMAP Grants in Sonoma County, 1995-2018') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')
  

```


## Plot loss results for iterative mitigation strategy

```{r}
## define target AAL for mitigation strategy
target <- strip(AAL['PARRA']/2)

## load "best-guess" estimate of number of elevations necessary
load('_results/mitigated/checkpoints/loss_bybldg.Rdata')
n.mitigate <- which(loss.bldg$new_loss <= loss.bldg$new_loss[1]/2)[1]

## load full mitigation results
files <- list.files('_results/mitigated/results', full.names = TRUE)
AAL_mitigated <- 
  foreach (file = files, .combine = 'rbind') %do% {
    load(file)
    c('n.bldg' = file %>% 
        str_remove('_results/mitigated/results/DV_') %>% 
        str_remove('.Rdata') %>% toNumber,
      'AAL' = sum(loss.sim$loss)/3200)
  } %>% data.frame %>% arrange(n.bldg)

## calculate 10-point moving average to smooth out variation
roll <- 10
AAL_mitigated$rollavg <- NA
for (i in 1:(nrow(AAL_mitigated)-roll)) {
  AAL_mitigated$rollavg[i+roll/2] <- mean(AAL_mitigated$AAL[i:(i+roll)])
}

## plot results
```

```{r echo = FALSE}
ggplot(AAL_mitigated) + 
  geom_point(aes(x = n.bldg, y = AAL)) + 
  scale_color_manual('AAL Target Values', values = roma.colors[2:1]) + 
  geom_hline(data = data.frame(z = AAL['PARRA']), aes(yintercept = z, color = 'Original AAL')) + 
  geom_hline(aes(yintercept = target, color = 'Mitigated AAL')) +
  ggnewscale::new_scale_color() + 
  scale_color_manual('Required \nMitigation', values = roma.colors[4]) + 
  geom_vline(data = data.frame(z = n.mitigate), 
             aes(xintercept = z, color = 'Rough Estimate'),
             linetype = 'dashed') +
  scale_x_origin('Number of Mitigated Homes') + 
  scale_y_origin(labels = comma_format(scale = 1e-6, prefix = '$', suffix = 'M'))

ggplot(AAL_mitigated) + 
  scale_color_manual('Simulation Results', values = c('black', 'grey70')) +
  geom_point(aes(x = n.bldg, y = AAL, color = 'Raw Data')) + 
  geom_line(aes(x = n.bldg, y = rollavg, color = '10-pt Rolling Average')) + 
  geom_hline(yintercept = target, color = roma.colors[2]) +
  geom_vline(xintercept = n.mitigate, color = roma.colors[4], linetype = 'dashed') +
  scale_x_continuous('Number of Mitigated Homes', limits = n.mitigate + 100*c(-1,1)) + 
  scale_y_origin(labels = comma_format(scale = 1e-6, prefix = '$', suffix = 'M'))

```

We calculated the mitigated AAL using two different methods: one that was quick but inexact (the "rough estimate"), and one that took longer but included the probabilistic uncertainty in the damage and loss component models. 
The rough estimate of the number of homes that needed to be mitigated in order to achieve the performance target was `r n.mitigate`. 
When we zoom in on the full results in the second plot this seems to be fairly accurate. 
We decided to choose 200 homes as a reasonable estimate of the mitigation threshold. 

```{r}
## define the number of buildings to mitigate
n.mitigate <- 200

## decide on optimal mitigation result
load('_results/mitigated/results/DV_200.Rdata')
loss.mitigated <- loss.sim

```


## Examine points of interest on the mitigated loss exceedance curve 

We revisit the numbers of interest we analyzed for the original loss exceedance curve and compute them again for the mitigated loss exceedance curve.
First we calculate the new return period for an event similar to the 2019 case study, then we calculate the new expected loss associated with the 100-year event.

### Estimate return period for the 2019 event using the mitigated loss exceedance curve

```{r echo = FALSE}
## calculate rate of occurrence for 2019 event
p <- sum(loss.mitigated$loss > 91.6e6)/3200

## show values as formatted table
data.frame(
  label = c('Annual Rate of Occurrence', 'Estimated Return Period'),
  value = c(p, round(1/p, 2))) %>% 
  gt %>% 
  fmt_markdown('label') %>% 
  fmt_number('value', n_sigfig = 4, drop_trailing_zeros = TRUE) %>% 
  cols_label(label = 'Parameter', value = 'Value') %>% 
  tab_header('Frequency of 2019 Event in Stochastic Record') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

### Estimate expected loss associated with the 100-year event using the mitigated loss exceedance curve

```{r}
## expected loss due to 1-in-100 year event
loss.mitigated %>% 
  arrange(desc(loss)) %>% 
  mutate(p = (1:nrow(.))/3200) %>% 
  mutate(RP = 1/p) %>% 
  filter(p == 0.01) %>%
  mutate(loss = loss/1e6) %>% pull(loss) %>% 
  comma(prefix = '$', suffix = 'M', accuracy = 0.01)

```


## Plot original vs. mitigated loss exceedance curve

Last but not least, we combine the original and mitigated loss exceedance curves into a single plot. 
This is included as Figure 10 in the paper.

```{r echo = FALSE}
lossexceedance <- lossexceedance %>% 
  mutate(mitigated = c(rep(0, nrow(.)-nrow(loss.mitigated)), sort(loss.mitigated$loss)))

```

```{r echo = FALSE}
ggplot(lossexceedance) + 
  geom_hline(yintercept = 1e-2, color = 'grey70', linetype = 'dashed') + 
  annotate('text', x = 275e6, y = 1e-2, hjust = 0, vjust = -0.5,
           label = '"100 Year Event"', fontface = 'italic', 
           family = 'Segoe UI', size = 7/.pt, color = 'grey70') + 
  geom_step(aes(x = stochastic, y = rp, color = 'Original'), size = 0.75) +
  geom_step(aes(x = mitigated, y = rp, color = 'Mitigated'), size = 0.75) +
  scale_color_manual('Building \nElevations', values = c('black', 'grey60'),
                     breaks = c('Original', 'Mitigated')) + 
  scale_x_origin('Loss Estimate ($M)', breaks = seq(0, 5e8, 5e7),
                 labels = comma_format(scale = 1e-6)) + 
  scale_y_log10('Rate of Occurrence, \u03bb', labels = scientific) +
  annotation_logticks(sides = 'l') + 
  theme(legend.position = c(0.75, 0.75))
ggsave('_figures/fig11_mitigated.png', width = 8.3, height = 6, units = 'cm')

```

```{r include = FALSE}
## what is the average horizontal difference between the two loss exceedance curves?
lossexceedance %>% 
  mutate(diff = stochastic-mitigated) %>% 
  ggplot() + 
  geom_hline(yintercept = 10^c(-4:0), color = 'grey90') +
  geom_point(aes(x = diff, y = rp)) + 
  scale_x_origin('Original - Mitigated Differential ($M)', labels = comma_format(scale = 1e-6)) + 
  scale_y_log10('Rate of Occurrence, \u03bb', labels = scientific) +
  annotation_logticks(sides = 'l')
  
```

