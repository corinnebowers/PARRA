---
title: "fit_surrogate"
output:
  html_document:
    toc: true 
    toc_float: true
    #toc_depth: 3  
    code_folding: hide
    number_sections: true 
    theme: spacelab   #https://www.datadreaming.org/post/r-markdown-theme-gallery/
    highlight: tango  #https://www.garrickadenbuie.com/blog/pandoc-syntax-highlighting-examples/
---

```{r setup, include = FALSE}
## setup
knitr::opts_knit$set(root.dir = 'D:/1-PARRA/')
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(results = 'hold', fig.show = 'hold', fig.align = 'center')
rm(list = ls())

```

```{r}
## setup information
source('_data/setup.R')
source('_data/plots.R')

## load required packages
require(plotly)
require(GGally)

## load location information
load('_data/lisflood/dem.Rdata')
load('_data/aoi/aoi.Rdata')
load('_data/NHD/NHD.Rdata')

## load nonzero cells
load('_data/nonzero/nonzero.Rdata')

```

```{r echo = FALSE}
## should figures be saved out for the publication?
publish <- FALSE

if (!publish) {
  theme_set(
    theme_classic() + theme(
      text = element_text(family = 'Segoe UI', size = 12),
      axis.line = element_line(size = 0.5),
      axis.ticks = element_line(size = 0.5, color = 'black'),
      legend.key.size = unit(0.5, 'cm')))
  }

```

# Set up best-fit parameter optimization problem

## Define the surrogate model

<!-- We have chosen to use the inverse distance weighted (IDW) spatial interpolation method.  -->
We used the inverse distance weighting (IDW) spatial interpolation method to generate inundation maps within the Monte Carlo process and reduce the computational demand of the PARRA framework. 
The IDW surrogate model is described by the three equations below.

$$ \mathrm{M}^* = \frac{\sum\limits_{i=1}^n \lambda_i \mathrm{M}_i} {\sum\limits_{i=1}^n \lambda_i}$$

$$ \lambda_i = \lVert \mathrm{\textbf{x}}^*, \mathrm{\textbf{x}}_i \rVert ^{-p} $$

$$ \mathrm{\textbf{x}} = \left\{ \alpha\!*\!z_{Q_p}, \; (1-\alpha)\!*\!z_{t_p} \right\} $$

$\mathrm{M}^*$ in the first equation represents the unknown (target) inundation map we are trying to predict. 
We calculate this map as the weighted sum of the $n$ closest maps. 
A map $\mathrm{M}_i$ is defined as "close" if its hydrograph parameters $\left\{Q_p, t_p \right\}_i$  are similar to the target hydrograph parameters $\left\{Q_p, t_p \right\}^*$.  

$Q_p^*$ and $t_p^*$ are known values used as inputs to the IDW interpolator function. 
However, they are not Euclidean coordinates, so $\mathrm{\textbf{x}}^*$ in the third equation is the coordinate vector that represents $\left\{Q_p, t_p \right\}^*$ in modified parameter space, after applying a normal score transformation and an anisotropy correction factor $\alpha$. 
We calculate the L2 (Euclidean) distance between $\mathrm{\textbf{x}}^*$, the coordinate vector of the target map, and $\mathrm{\textbf{x}}_i$, the coordinate vector of the $i^{th}$ closest map. 
The inverse of this distance multiplied by the power function $p$ is the calculated weight $\lambda_i$ for map $\mathrm{M}_i$, as shown in the second equation.

## Define hyperparameters

The IDW method has three hyperparameters to tune, which are as follows:

* $n$ defines the size of the search neighborhood;
* $p$ is the power function coefficient, which defines the decay rate of the distance weighting; and
* $\alpha$ defines the anisotropy (asymmetry of information content) between $Q_p$ and $t_p$.


## Define error metric

We generate a bank of LISFLOOD models for various combinations of $Q_p$ and $t_p$. 
We then find the best-fit hyperparameters by performing 10-fold cross-validation on Sherlock, Stanford's high-performance computing cluster.
Within each fold of the 10-fold cross-validation, 10\% of the data is withheld for testing.
The surrogate model predicts the unseen data, and the error metric is some measurement of the difference between the observed vs. predicted inundation map. 
The goal of the cross-validation is to find the set of hyperparameters that minimizes the error. 

A cell-by-cell comparison of inundation predictions at every 40 m $times$ 40 m cell in the study area is computationally intractable.
Therefore we calculate aggregate error across the study area domain in two ways.
The RMSE (root mean squared error) is a common error metric used in machine learning and parameter tuning. 
It is attractive because it includes all data points while also penalizing larger error values. 
The MAE (mean absolute error) is a less common but more intuitive metric that weights all error values equally and measures how far your predictions were off, on average, from the true value.
We then repeat this calculation two times: once on the cells that experience inundation at least 1\% of the time (10 of the pre-computed LISFLOOD maps) and once on the cells that experience inundation at least 50\% of the time. 
This is to focus in on the locations of the study area that contribute the most to severe flood damage and flood impacts.
The extent of the study area inundation 1\% of the time vs. 50\% of the time is shown in the figure below.

```{r}
## plot 1% inundation area vs. 50% inundation area
ggplot(raster.df(nonzero)) + 
  geom_sf(data = sonoma %>% st_union %>% st_crop(aoi), 
          color = 'black', fill = 'grey95') + 
  # geom_sf(data = aoi, fill = NA) + 
  geom_raster(data = raster.df(nonzero) %>% filter(value >= 10),
              aes(x=x, y=y, fill = '1% Inundation Area')) + 
  geom_raster(data = raster.df(nonzero) %>% filter(value >= 500),
              aes(x=x, y=y, fill = '50% Inundation Area')) +
  scale_fill_manual('', values = c('grey70', 'grey30')) + 
  geom_sf(data = russian %>% st_crop(aoi)) + 
  theme_void() + theme(legend.position = 'bottom')

```

Overall this gives us four error metrics to consider.
However, these metrics are just to move from a full map to a single summary number. 
The results of the Sherlock computing give us 1,000 values for each combination of $n$, $p$, and $\alpha$, one for every pre-computed map. 
This is shown in the histogram below for a single hyperparameter combination. 
Therefore we aggregate our error metrics even further, as described in the next section.

```{r}
## load error data
files <- paste0('_scripts/5_INUN/fit_inundation/5e_fit_surrogate/results/error', 1:10, '.csv')
error <- foreach(file = files) %do% {
  read_csv(file)} %>%
  do.call(rbind, .)

# const01 <- (error$SRSS01/error$RMSE01)[1]
# const50 <- (error$SRSS50/error$RMSE50)[1]
# error <- error %>% 
#   mutate(MAE01 = MAE01/const01^2, MAE50 = MAE50/const50^2)

```

```{r}
## plot error histogram for one hyperparameter combination
error %>% 
  filter(n == 5 & p == 1 & alpha == 0.6) %>% 
  arrange(desc(RMSE50)) %>% 
  ggplot() + 
  geom_density(aes(x = MAE01, color = 'MAE01'), size = 1) + 
  geom_density(aes(x = RMSE01, color = 'RMSE01'), size = 1) + 
  geom_density(aes(x = MAE50, color = 'MAE50'), size = 1) + 
  geom_density(aes(x = RMSE50, color = 'RMSE50'), size = 1) + 
  scale_color_manual('Error Metric', values = roma.colors[-3]) + 
  scale_x_origin() + scale_y_origin() + 
  coord_cartesian(xlim = c(0,0.5)) + 
  theme(legend.position = c(0.8, 0.8))

```

# Calculate best-fit IDW parameter values

For each MAE metric we calculate the mean, 5th percentile, and 95th percentile values.
For each RMSE metric we calculate the square root of mean of squares (rms), 5th percentile, and 95th percentile values.
Rather than the rigorous parameter fit process we conducted for the LISFLOOD environmental parameters in '5a_fit_lisflood', this is more of a qualitative assessment. 

## Identify candidate metric pairs

We first start by identifying pairs of metrics that have low correlation, i.e. contain less overlapping and more distinct information. 

```{r}
## summarize metric values by parameters
error.npalpha <- error %>% 
  group_by(n, p, alpha) %>% 
  summarize(
    MAE01.mean = mean(MAE01), MAE50.mean = mean(MAE50),
    MAE01.05 = quantile(MAE01, 0.05), MAE01.95 = quantile(MAE01, 0.95),
    MAE50.05 = quantile(MAE50, 0.05), MAE50.95 = quantile(MAE50, 0.95),
    RMSE01.rms = sqrt(mean(RMSE01^2)), RMSE50.rms = sqrt(mean(RMSE50^2)),
    RMSE01.05 = quantile(RMSE01, 0.05), RMSE01.95 = quantile(RMSE01, 0.95),
    RMSE50.05 = quantile(RMSE50, 0.05), RMSE50.95 = quantile(RMSE50, 0.95)) %>% 
  ungroup

## plot cross-correlation between metrics
error.npalpha %>% select(starts_with('MAE')) %>% ggpairs
error.npalpha %>% select(starts_with('RMSE')) %>% ggpairs
error.npalpha %>% select(contains('01')) %>% ggpairs
error.npalpha %>% select(contains('50')) %>% ggpairs

```

## Find best-fit parameters for different metric pairs

Using the interactive plotly tool in R, we iteratively removed values of $n$, $p$, and $\alpha$ that led to unfavorable error outcomes. 
When the size of the best-fit subset reached 1-2\% or less of the total number of parameter combinations, we saved the best-fit $n$, $p$ and $\alpha$ values in a vector.
We repeated this process for a few different combinations of metrics and compared the best-fit parameter values found by each combination.

```{r}
## initialize empty vectors to store best-fit values across all simulations
n.best <- c()
p.best <- c()
alpha.best <- c()

```

```{r}
## balance MAE01.mean & RMSE50.rms
## (note: filter commands were added one-by-one to achieve the final results)

error.subset <- error.npalpha %>% 
  filter(alpha > 0.25 & alpha < 0.85) %>% 
  filter(n > 3 & n < 20) %>% 
  filter(p > 0.75 & p < 4) %>% 
  filter(alpha > 0.35 & alpha < 0.75) %>% 
  filter(n < 10) %>% 
  filter(p > 1.5) %>% 
  filter(n > 4 & alpha > 0.45)
nrow(error.subset)/nrow(error.npalpha)

# plot_ly(
#   data = error.subset,
#   x = ~MAE01.mean, y = ~RMSE50.rms, color = ~factor(n),
#   colors = scico(n = length(unique(error.subset$n)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~MAE01.mean, y = ~RMSE50.rms, color = ~factor(p),
#   colors = scico(n = length(unique(error.subset$p)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~MAE01.mean, y = ~RMSE50.rms, color = ~factor(alpha),
#   colors = scico(n = length(unique(error.subset$alpha)), palette = 'roma'))
  
n.best <- c(n.best, unique(error.subset$n))
p.best <- c(p.best, unique(error.subset$p))
alpha.best <- c(alpha.best, unique(error.subset$alpha))

```

```{r}
## balance MAE50.mean & RMSE01.rms

error.subset <- error.npalpha %>% 
  filter(alpha > 0.3 & alpha < 0.75) %>% 
  filter(n > 3 & n < 12) %>% 
  filter(p > 1 & p < 4) %>% 
  filter(alpha > 0.45 & alpha < 0.7) %>% 
  filter(n > 4 & n < 10) %>% 
  filter(p > 1.5 & alpha > 0.5)
nrow(error.subset)/nrow(error.npalpha)

# plot_ly(
#   data = error.subset,
#   x = ~MAE50.mean, y = ~RMSE01.rms, color = ~factor(n),
#   colors = scico(n = length(unique(error.subset$n)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~MAE50.mean, y = ~RMSE01.rms, color = ~factor(p),
#   colors = scico(n = length(unique(error.subset$p)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~MAE50.mean, y = ~RMSE01.rms, color = ~factor(alpha),
#   colors = scico(n = length(unique(error.subset$alpha)), palette = 'roma'))

n.best <- c(n.best, unique(error.subset$n))
p.best <- c(p.best, unique(error.subset$p))
alpha.best <- c(alpha.best, unique(error.subset$alpha))

```

```{r}
## balance RMSE01.95 & RMSE50.05

error.subset <- error.npalpha %>% 
  filter(n > 2 & n < 15) %>% 
  filter(p < 4) %>% 
  filter(alpha > 0.35 & alpha < 0.75) %>% 
  filter(p > 0.5) %>% 
  filter(n > 3 & n < 12) %>% 
  filter(alpha > 0.45 & alpha < 0.7) %>% 
  filter(p > 1) %>% 
  filter(n > 4 & n < 10) %>% 
  filter(alpha > 0.5)
nrow(error.subset)/nrow(error.npalpha)

# plot_ly(
#   data = error.subset,
#   x = ~RMSE01.95, y = ~RMSE50.05, color = ~factor(n),
#   colors = scico(n = length(unique(error.subset$n)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~RMSE01.95, y = ~RMSE50.05, color = ~factor(p),
#   colors = scico(n = length(unique(error.subset$p)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~RMSE01.95, y = ~RMSE50.05, color = ~factor(alpha),
#   colors = scico(n = length(unique(error.subset$alpha)), palette = 'roma'))

n.best <- c(n.best, unique(error.subset$n))
p.best <- c(p.best, unique(error.subset$p))
alpha.best <- c(alpha.best, unique(error.subset$alpha))

```

```{r}
## balance MAE01.95 & MAE50.05

error.subset <- error.npalpha %>% 
  filter(alpha > 0.35 & alpha < 0.85) %>% 
  filter(n > 1 & n < 10) %>% 
  filter(p > 1 & p < 4) %>% 
  filter(alpha > 0.5 & alpha < 0.8) %>% 
  filter(n > 4) %>% 
  filter(alpha > 0.55 & alpha < 0.75) %>% 
  filter(n < 8) %>% 
  filter(p < 3)
nrow(error.subset)/nrow(error.npalpha)

# plot_ly(
#   data = error.subset,
#   x = ~MAE01.95, y = ~MAE50.05, color = ~factor(n),
#   colors = scico(n = length(unique(error.subset$n)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~MAE01.95, y = ~MAE50.05, color = ~factor(p),
#   colors = scico(n = length(unique(error.subset$p)), palette = 'roma'))
# plot_ly(
#   data = error.subset,
#   x = ~MAE01.95, y = ~MAE50.05, color = ~factor(alpha),
#   colors = scico(n = length(unique(error.subset$alpha)), palette = 'roma'))

n.best <- c(n.best, unique(error.subset$n))
p.best <- c(p.best, unique(error.subset$p))
alpha.best <- c(alpha.best, unique(error.subset$alpha))

```

## Consolidate & summarize results 

Here are the values that consistently emerged as best-fit values for the three parameters under consideration:

```{r echo = FALSE}
n.best <- sort(unique(n.best))
p.best <- sort(unique(p.best))
alpha.best <- sort(unique(alpha.best))

cat('n.best:\n'); print(n.best)
cat('\np.best:\n'); print(p.best)
cat('\nalpha.best:\n'); print(alpha.best)

error.best <- error.npalpha %>% 
  filter(n %in% n.best & p %in% p.best & alpha %in% alpha.best)

```

These represent `r nrow(error.best)` parameter combinations, or `r scales::percent(nrow(error.best)/nrow(error.npalpha), accuracy = 0.1)` of all parameter combinations considered. 
At this point, the user could choose to use all of these as uniformly weighted options in the surrogate model to represent parameter uncertainty.
However, we move one step further and define single point values as the defaults for the *generate_inundation* function in the file `INUN.R`. 
After a holistic evaluation of the multiple error metrics, we chose the following values.

```{r include = FALSE}
# error.best %>% 
#   select(-n, -p, -alpha) %>% 
#   apply(2, function(x) x/max(x)) %>% 
#   apply(1, sum) %>% 
#   cbind(error.best %>% select(n, p, alpha), error.total = .) %>% 
#   arrange(error.total)
# error.best %>% 
#   select(-n, -p, -alpha) %>% 
#   apply(2, function(x) x/max(x)) %>% 
#   as.data.frame %>% 
#   mutate(central = select(., ends_with('mean'), ends_with('rms')) %>% apply(1, sum),
#          extreme = select(., ends_with('05'), ends_with('95')) %>% apply(1, sum)) %>% 
#   transmute(error.weighted = central*2/3 + extreme/3) %>% 
#   cbind(error.best %>% select(n, p, alpha), .) %>% 
#   arrange(error.weighted)

```

```{r echo = FALSE}
data.frame(
  param = c('n', 'p', 'alpha'),
  value = c(6, 2, 0.7)) %>% 
  gt %>% 
  fmt_markdown('param') %>% 
  fmt_number('value') %>%
  cols_label(param = 'Parameter', value = 'Best-Fit Value') %>% 
  tab_header('Surrogate Model Parameter Values') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')

```

# Calculate surrogate model error

We pull the 1,000 simulations that were run with the best-fit parameters and assess the error. 
The average values of our error metrics of interest are shown in the table below. 
Additionally, we take the metric with the largest average (RMSE50) and the smallest average (MAE01) and plot the full error distributions as histograms. 

```{r}
## find all simulations run with best-fit parameters
error.final <- error %>% filter(n == 6 & p == 2 & alpha == 0.7)

## show average error metrics as a table
error.final %>% 
  summarize(RMSE01 = sqrt(mean(RMSE01^2)), RMSE50 = sqrt(mean(RMSE50^2)),
            MAE01 = mean(abs(MAE01)), MAE50 = mean(abs(MAE50))) %>% 
  t %>% 
  data.frame(val = .) %>% 
  rownames_to_column(var = 'name') %>%
  mutate(val = val*100) %>% 
  gt %>% 
  fmt_markdown('name') %>% 
  fmt_number('val') %>% 
  cols_label(name = 'Error Metric', val = 'Value (cm)') %>% 
  tab_header('Surrogate Model Average Error') %>% 
  tab_options(heading.background.color = '#d9d9d9', 
              column_labels.background.color = '#f2f2f2')
  
## plot MAE01 and RMSE50
```

```{r echo = FALSE}
ggplot(error.final) + 
  geom_histogram(aes(MAE01), color = 'black', fill = 'grey90', 
                 bins = sqrt(nrow(error.final)), boundary = 0) + 
  ggtitle('Surrogate Model Error (MAE)', 
          subtitle = 'calculated for cells that flood at least 1% of simulations') + 
  scale_x_origin('Mean Absolute Error') + 
  scale_y_origin('Number of Occurrences')

ggplot(error.final) + 
  geom_histogram(aes(RMSE50), color = 'black', fill = 'grey90', 
                 bins = sqrt(nrow(error.final)), boundary = 0) + 
  ggtitle('Surrogate Model Error (RMSE)', 
          subtitle = 'calculated for cells that flood at least 50% of simulations') + 
  scale_x_origin('Root Mean Squared Error') + 
  scale_y_origin('Number of Occurrences')

```

Almost all simulations have an MAE01 of 10cm or less and a RMSE50 of 50cm or less, and all of the averages are well within the vertical tolerance of the DEM we are using to generate inundation maps. 
Therefore we are very satisfied with the accuracy of the surrogate model.


